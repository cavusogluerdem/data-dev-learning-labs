<!DOCTYPE html><html><head><meta charset="utf-8"><title>Untitled Document.md</title><style></style></head><body id="preview">
<h1><a id="VIDEO_ANALYTICS_USING_APACHE_SPARK_STREAMING_0"></a>VIDEO ANALYTICS USING APACHE SPARK STREAMING</h1>
<p><strong>Processing Engine Used</strong> : Apache Spark</p>
<p><strong>Stream Processing Platform Used</strong>: Apache Kafka</p>
<p><strong>Executed on</strong>: Hadoop Big Data Platform</p>
<h2><a id="OVERVIEW_8"></a>OVERVIEW</h2>
<p>This learning lab can be used as a guide to get a high level understanding of video analytics using Apache Spark streaming.<br>
The application mentioned here uses micro service for video data ingestion which reads the video data from mockup video source and forwards the data to Kafka broker. The sample code shown in this learning lab reads the video data from Kafka broker which detects the face image(s) and saves those image(s) to HDFS platform. We will be using DevNet Data Learning Platform referred as “DLP” during the course. The User can view the face image(s) from DLP platform.</p>
<p>In this lab, we integrate Apache Spark Streaming with Kafka, detect a human face using OpenCV from a video streaming and then save the result to an image file in Hadoop environment. We have used a mockup video source for demonstrating an example. In real time, user can use a live video streaming from a real camera and write a video source service for reading the live streaming data and send the data from the camera to Kafka thereby replacing  the mockup video source.</p>
<p>Please refer the screenshot below for a high level understanding:<br>
<img src="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/VIDEO%20ANALYTICS%20USING%20APACHE%20SPARK%20STREAMING/assets/mockVideoSource.png?raw=true" alt="alt-tag"></p>
<h2><a id="PREREQUISITES_20"></a>PRE-REQUISITES</h2>
<ol>
<li>
<p>Install Chrome Browser.</p>
</li>
<li>
<p>Basic understanding of Apache Hadoop and Big Data.</p>
</li>
<li>
<p>Basic knowledge of Java and Maven.</p>
</li>
<li>
<p>Basic knowledge of OpenCV faces detection function.</p>
</li>
<li>
<p>Basic understanding of how Spark Streaming works.</p>
</li>
<li>
<p>Obtain access to Data Learning Platform -<br>
Refer: <a href="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/HowToAccessDTLP.md">https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/WORD COUNT USING SCALA WITH APACHE SPARK/HowToAccessDTLP.md</a></p>
</li>
</ol>
<h2><a id="LEARNING_OBJECTIVES_36"></a>LEARNING OBJECTIVES</h2>
<ol>
<li>
<p>To get familiarized with the DLP (Data Learning Platform)</p>
</li>
<li>
<p>To get familiarized with Apache Spark Streaming integration with Kafka.</p>
</li>
<li>
<p>To get familiarized with the technique to perform video analytic functions in a real-time stream application.</p>
</li>
<li>
<p>To get familiarized with the technique to save a file to HDFS.</p>
</li>
</ol>
<h2><a id="TERMINOLOGIES_USED_47"></a>TERMINOLOGIES USED</h2>
<h3><a id="APACHE_SPARK__AN_INTRODUCTION_49"></a>APACHE SPARK - AN INTRODUCTION</h3>
<p>Apache Spark is an open source cluster computing framework. Spark is advertised as “lightning fast cluster computing”. It has a thriving open-source community and is the most active Apache project at the moment. Spark provides an interface for programming entire clusters with implicit data parallelism and fault-tolerance. Apache Spark provides programmers with an application programming interface centered on a data structure called the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.</p>
<p>It was developed in response to limitations in the MapReduce cluster computing paradigm, which forces a particular linear dataflow structure on distributed programs. MapReduce programs read input data from disk, map a function across the data, reduce the results of the map, and store reduction results on disk. Spark provides a faster and more general data processing platform.</p>
<h6><a id="What_is_Spark_Streaming_55"></a>What is Spark Streaming?</h6>
<p>Spark Streaming is a key component of Spark, a massively successful open source project in the recent years. Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map-reduce, join and window. Finally, processed data can be pushed out to file systems, databases, and live dashboards.</p>
<h6><a id="Key_Features_59"></a>Key Features</h6>
<ol>
<li>Currently provides APIs in Scala, Java, and Python, with support for other languages (such as R) on the way</li>
<li>Integrates well with the Hadoop ecosystem and data sources (HDFS, Amazon S3, Hive, HBase, Cassandra, etc.)</li>
<li>Can run on clusters managed by Hadoop YARN or Apache Mesos, and can also run standalone</li>
</ol>
<h6><a id="How_to_Use_Apache_Spark_65"></a>How to Use Apache Spark?</h6>
<h6><a id="Example__Using_spark_to_detect_an_earthquake_by_analyzing_the_twitter_stream_67"></a>Example : Using spark to detect an earthquake by analyzing the twitter stream</h6>
<ol>
<li>
<p>Using Spark streaming, filter tweets that seem relevant like “earthquake” or “shaking”.</p>
</li>
<li>
<p>Run semantic analysis on the tweets to determine if they appear to be referencing a current earthquake occurrence. Tweets like “Earthquake!” or ”Now it is shaking”, for example, would be considered positive matches, whereas tweets like “Attending an Earthquake Conference” or ”The earthquake yesterday was scary” would not.</p>
</li>
<li>
<p>Using the streaming technique we could detects the positive tweets in a defined time window and thereby can be used to send alert messages.</p>
</li>
</ol>
<p>For more details, please refer:<br>
<a href="http://spark.apache.org/">http://spark.apache.org/</a><br>
<a href="https://en.wikipedia.org/wiki/Apache_Spark">https://en.wikipedia.org/wiki/Apache_Spark</a><br>
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">http://spark.apache.org/docs/latest/streaming-programming-guide.html</a></p>
<h2><a id="APACHE_KAFKA__AN_INTRODUCTION_81"></a>APACHE KAFKA? - AN INTRODUCTION</h2>
<p>Kafka is a distributed streaming platform that is designed to be fast, scalable, and durable. It has 3 key capabilities:</p>
<ol>
<li>It lets you publish and subscribe to streams of records. In this respect it is similar to a message queue or enterprise messaging system.</li>
<li>It lets you store streams of records in a fault-tolerant way.</li>
<li>It lets you process streams of records as they occur.</li>
</ol>
<p>It gets used for two broad classes of application:</p>
<ol>
<li>Building real-time streaming data pipelines that reliably get data between systems or applications</li>
<li>Building real-time streaming applications that transform or react to the streams of data</li>
</ol>
<p>For more details, please refer:<br>
<a href="https://kafka.apache.org/">https://kafka.apache.org/</a></p>
<h3><a id="HADOOP__AN_INTRODUCTION_98"></a>HADOOP - AN INTRODUCTION</h3>
<p>Apache Hadoop is an open-source software framework for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. To understand Hadoop, there are two fundamental things about it -  How Hadoop stores files and how it processes data. The framework that is used in Hadoop to process data is called MapReduce.</p>
<p>All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common and should be automatically handled by the framework. The core of Apache Hadoop consists of a storage part, known as Hadoop Distributed File System (HDFS), and a processing part called MapReduce. Hadoop splits files into large blocks and distributes them across nodes in a cluster.</p>
<p>Example: Imagine a file that is larger than the capacity of a computer then it would not be possible to store that file. Hadoop allows to store files bigger than what can be stored on one particular node or server. So it provides an ability to store very, very large files and also lets to store many, many files.</p>
<p>For more details, please refer:<br>
(<a href="https://en.wikipedia.org/wiki/Apache_Hadoop">https://en.wikipedia.org/wiki/Apache_Hadoop</a>)</p>
<h2><a id="PROCESS_OVERVIEW_110"></a>PROCESS OVERVIEW</h2>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/assets/images/Process4.jpeg?raw=true" alt="alt-tag"></p>
<p>Please follow the steps given below to launch the workspace and execute the program.</p>
<h3><a id="Step_1__Select_Learning_Lab_from_DLP_116"></a><strong>Step 1 : Select Learning Lab from DLP</strong></h3>
<p>After launching the DLP dashboard page, navigate to learning labs tab. Select the learning lab “Video Analytics using Apache Spark Streaming” and click on “Start” button as shown in screenshot below:</p>
<p><img src="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/VIDEO%20ANALYTICS%20USING%20APACHE%20SPARK%20STREAMING/assets/VideoAnalytics12.jpeg?raw=true" alt="alt-tag"></p>
<h3><a id="Step_2__Workspace_Page_122"></a><strong>Step 2 : Workspace Page</strong></h3>
<p>On click of Start button, user will be navigated to a workspace page where all the components - IDE, Tools and Microservices required to execute the program are available. Please refer the screenshot below:</p>
<p><img src="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/VIDEO%20ANALYTICS%20USING%20APACHE%20SPARK%20STREAMING/assets/VideoAnalytics9.jpeg?raw=true" alt="alt-tag"></p>
<h3><a id="Step_3__Input_Configuration_128"></a><strong>Step 3 : Input Configuration</strong></h3>
<p>Click on the configuration settings button provided in “Tools and Microservices” column.Please refer screenshot below. The input configuration existing for the learning lab is shown.</p>
<p><img src="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/VIDEO%20ANALYTICS%20USING%20APACHE%20SPARK%20STREAMING/assets/VideoAnalytics21.jpeg?raw=true" alt="alt-tag"></p>
<h3><a id="Step_4__Start_service_in_workspace_134"></a><strong>Step 4 : Start service in workspace</strong></h3>
<p>Points to Note:</p>
<ul>
<li>Start Cloud IDE(where eclipse icon is shown) service as shown in screenshot below:</li>
</ul>
<p><img src="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/VIDEO%20ANALYTICS%20USING%20APACHE%20SPARK%20STREAMING/assets/VideoAnalytics10.jpeg?raw=true" alt="alt-tag"></p>
<ul>
<li>Once started, the colour of the icon changes to Green and launch button is enabled. Please refer screenshot below:</li>
</ul>
<p><img src="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/VIDEO%20ANALYTICS%20USING%20APACHE%20SPARK%20STREAMING/assets/VideoAnalytics8.jpeg?raw=true" alt="alt-tag"></p>
<ul>
<li>if the service is in started status initially, please stop it first and start it again as detailed above and If the service is in stopped status then, click on it to start again.</li>
</ul>
<h3><a id="Step_5__Launch_IDE_148"></a><strong>Step 5 : Launch IDE</strong></h3>
<ul>
<li>Click launch on cloud IDE pane (where eclipse icon is shown) and user will be navigated to a pre-configured IDE (Integrated Development Environment) as shown in the screenshot below:</li>
</ul>
<p><img src="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/VIDEO%20ANALYTICS%20USING%20APACHE%20SPARK%20STREAMING/assets/VideoAnalytics1.png?raw=true" alt="alt-tag"></p>
<ul>
<li>Click on the left menu tree - VideoLabs folder under project explorer. The folder will expand and show all the files underneath. Navigate to VideoLabs -&gt; src -&gt; main -&gt;java -&gt;ddp -&gt;VideoLabs. The folder structure is as shown in screenshot below:</li>
</ul>
<p><img src="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/VIDEO%20ANALYTICS%20USING%20APACHE%20SPARK%20STREAMING/assets/VideoAnalytics11.png?raw=true" alt="alt-tag"></p>
<p>Select the file “App.java” from the path shown above. The below mentioned code snippet would be shown to the user.</p>
<p>The Java code mentioned below is used to detect the video streaming from a source. It reads the video data, detects the face and then saves the face images back to HDFS.</p>
<pre><code class="language-json">package DDP.VideoLabs;

//import java libraries
import java.io.BufferedInputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.net.URI;
import java.util.Arrays;
import java.util.HashMap;
import java.util.HashSet;
import java.util.UUID;
import java.security.PrivilegedExceptionAction;
import org.joda.time.DateTime;

//import spark libraries
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka.KafkaUtils;

//import opencv libraries
import org.opencv.core.Core;
import org.opencv.core.Mat;
import org.opencv.core.MatOfRect;
import org.opencv.core.Size;
import org.opencv.imgcodecs.Imgcodecs;
import org.opencv.imgproc.Imgproc;
import org.opencv.objdetect.CascadeClassifier;

//import kafka libraries
import kafka.serializer.DefaultDecoder;
import kafka.serializer.StringDecoder;
import scala.Tuple2;

//import hadoop libraries
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;

public class App 
{
    
    public static void main( String[] args ) throws InterruptedException
    {
        if (args.length != 5)
        {
            System.out.println("please set parameter value for kafka address, kafka topic,hadoop username,hdfs path and running application duration");
            return;
        }
        //set parameter value for kafka, topic, username, hdfs path and running application duration
        String kafka = args[0];
        String topics = args[1];
        final String hdfsUsername = args[2];
        final String path = args[3];
        int duration = Integer.parseInt(args[4]);
        
        //Integrate the Spark Streaming with Kafka
        SparkConf sparkConf = new SparkConf().setAppName("JavaVideoLabs");
        final JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.seconds(2));

        HashSet&lt;String&gt; topicsSet = new HashSet&lt;String&gt;(Arrays.asList(topics.split(",")));
        HashMap&lt;String, String&gt; kafkaParams = new HashMap&lt;String, String&gt;();
        kafkaParams.put("metadata.broker.list", kafka);
        kafkaParams.put("group.id", "groupid");
        kafkaParams.put("consumer.id", "consumerid");
    
        //Get the video data from kafka topic
        JavaPairInputDStream&lt;String, byte[]&gt; messages = KafkaUtils.createDirectStream(
                jssc,
                String.class,
                byte[].class,
                StringDecoder.class,
                DefaultDecoder.class,
                kafkaParams,
                topicsSet
        );
    
        //Detect the face from video data and save the face image to HDFS          
        JavaDStream&lt;String&gt; content = messages.map(new Function&lt;Tuple2&lt;String, byte[]&gt;, String&gt;() {
            private static final long serialVersionUID = 1L;
            //@Override
            public String call(Tuple2&lt;String, byte[]&gt; tuple2) throws IOException {
                System.loadLibrary(Core.NATIVE_LIBRARY_NAME);
                if ((tuple2 == null) || (tuple2._2().length &lt; 1000))
                    return "";
                
                Mat image = new Mat(new Size(640, 480), 16);
                image.put(0, 0, tuple2._2()); 
                                
                // Detect faces in the image using OpenCV.
                Mat mGrey = new Mat();
                Imgproc.cvtColor( image, mGrey, Imgproc.COLOR_BGR2GRAY); 
                CascadeClassifier faceDetector =
                        new CascadeClassifier(GetResourceFilePath("/haarcascade_frontalface_alt.xml").toString());
                MatOfRect faceDetections = new MatOfRect();
                faceDetector.detectMultiScale(mGrey, faceDetections);
                int len = faceDetections.toArray().length;
                System.out.println(String.format("Detected %s faces", len));
                if (len &gt; 0)
                {
                    SaveImageToHDFS(image, hdfsUsername, path);
                    return "face";
                  
                } else
                    return "";
           
            }
        });
        
        //The application will stop execution when its run time is bigger than the time which has passed as parameter "duration" 
        DateTime start = new DateTime();
        
        content.count().print();
       
        jssc.start();
        DateTime end;
        while (true) {
            end = new DateTime();
            if (end.getMillis()-start.getMillis() &gt; duration * 60000) {
                jssc.stop(true, true);
                break;
            }
            Thread.sleep(2000);            
        }

        jssc.awaitTermination();
    }
    
    public static void SaveImageToHDFS(final Mat img,final String hdfsUsername, final String hdfsPath) throws IOException
    {
        try
        {
            UserGroupInformation ugi = UserGroupInformation.createRemoteUser(hdfsUsername);

            ugi.doAs(new PrivilegedExceptionAction&lt;Void&gt;() {

            public Void run() throws Exception {
                //Save the face image as a temporary file
                String tmpFile = "/tmp/" + UUID.randomUUID().toString() + ".jpg";
                Imgcodecs.imwrite(tmpFile, img);
            
                //Save the temporary file for face image to HDFS
                String hdfsAddr = hdfsPath+"/"+"face.jpg";
                Configuration config = new Configuration();
                config.set("hadoop.job.ugi", hdfsUsername);
                FileSystem fs = FileSystem.get(URI.create(hdfsAddr), config);
                Path path = new Path(hdfsAddr);
                FSDataOutputStream out = fs.create(path, true);
                InputStream is = new BufferedInputStream(new FileInputStream(tmpFile));
                IOUtils.copyBytes(is, out, config);
                is.close();
                out.close();
                fs.close();
               
                //Delete the temporary file
                File f = new File(tmpFile);
                if (f.exists() &amp;&amp; !f.isDirectory())
                    f.delete();
                System.out.println("write file to hdfs");
                return null;
            }
        });
        } catch (Exception e) {
            e.printStackTrace();
        }
 }
    
    //Read the resource file from the jar file then save it as temporary file
    public static String GetResourceFilePath(String filename) {
        InputStream inputStream = null;
        OutputStream outputStream = null;
        String tempFilename = "/tmp" + filename;
        try {
            File f = new File(tempFilename);
            if (f.exists() &amp;&amp; !f.isDirectory())
                return tempFilename;
            
            // read this file into InputStream
            inputStream = App.class.getResourceAsStream(filename);
            if (inputStream == null)
                System.out.println("empty streaming");
            // write the inputStream to a FileOutputStream
            outputStream =
                    new FileOutputStream(tempFilename);

            int read;
            byte[] bytes = new byte[102400];

            while ((read = inputStream.read(bytes)) != -1) {
                outputStream.write(bytes, 0, read);
            }
            outputStream.flush();

            System.out.println("Load XML file, Done!");

        } catch (IOException e) {
            e.printStackTrace();
        } finally {
            if (inputStream != null) {
                try {
                    inputStream.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
            if (outputStream != null) {
                try {
                    // outputStream.flush();
                    outputStream.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }

            }

        }
        return tempFilename;
    }
    
}
</code></pre>
<h3><a id="Step_6__Package_the_program_395"></a><strong>Step 6 : Package the program</strong></h3>
<p>Select CMD as “Package” and click on blue icon (<img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/assets/images/icon.jpeg?raw=true" alt="alt-tag">) as shown in the screenshot below:</p>
<p><img src="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/VIDEO%20ANALYTICS%20USING%20APACHE%20SPARK%20STREAMING/assets/VideoAnalytics3.png?raw=true" alt="alt-tag"></p>
<p>Note : This will help to build and package the Java program.</p>
<p>Check the console as shown below for a successful completion message:</p>
<p><img src="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/VIDEO%20ANALYTICS%20USING%20APACHE%20SPARK%20STREAMING/assets/VideoAnalytics4.png?raw=true" alt="alt-tag"></p>
<h3><a id="Step_7__Run_the_program_407"></a><strong>Step 7 : Run the program</strong></h3>
<p>Select the CMD as “run” and click on blue icon(<img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/assets/images/icon.jpeg?raw=true" alt="alt-tag">) as shown in the screenshot below:</p>
<p><img src="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/VIDEO%20ANALYTICS%20USING%20APACHE%20SPARK%20STREAMING/assets/VideoAnalytics6.png?raw=true" alt="alt-tag"></p>
<p>The script (<a href="http://run.sh">run.sh</a>) submits the spark task to cluster. There are  the primarily five Input parameters that are used in <a href="http://run.sh">run.sh</a> which are as follows:</p>
<p><strong>Parameter 1</strong> : Kafka Address -&gt; Kafka broker IP Address<br>
<strong>Parameter 2</strong> : Kafka Topic -&gt; Kafka Topic text<br>
<strong>Parameter 3</strong> : $HADOOP_USER_NAME -&gt; login username<br>
<strong>Parameter 4</strong> : Hadoop Path -&gt; HDFS Path where the face image would be stored. The program detects face from streaming data and saves the images back to HDFS path specified<br>
<strong>Parameter 5</strong> : Run Duration :  Duration for which the application would run. By default, it is set as 5 mins. However, It could be changed by the user.</p>
<p><img src="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/VIDEO%20ANALYTICS%20USING%20APACHE%20SPARK%20STREAMING/assets/VideoAnalytics13.png?raw=true" alt="alt-tag"></p>
<p>On running the program, the console looks as shown below:<br>
<img src="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/VIDEO%20ANALYTICS%20USING%20APACHE%20SPARK%20STREAMING/assets/VideoAnalytics19.jpeg?raw=true" alt="alt-tag"></p>
<h3><a id="Step_8__View_output_images_from_DLP_426"></a><strong>Step 8 : View output images from DLP</strong></h3>
<p>Next, we will view the file that has face images from DLP Platform. The spark task detected the images from the video streaming and saved them into the HDFS path (mentioned in <a href="http://run.sh">run.sh</a> - Refer step 13 for more details).</p>
<ul>
<li>Navigate to Data Sources -&gt; File List. The output file generated will be listed. Please refer the screenshot below for more  details:</li>
</ul>
<p><img src="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/VIDEO%20ANALYTICS%20USING%20APACHE%20SPARK%20STREAMING/assets/VideoAnalytics18.jpeg?raw=true" alt="alt-tag"></p>
<ul>
<li>Select the preview button to view the image along with the timestamp:</li>
</ul>
<p><img src="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/VIDEO%20ANALYTICS%20USING%20APACHE%20SPARK%20STREAMING/assets/VideoAnalytics17.jpeg?raw=true" alt="alt-tag"></p>
<ul>
<li>Select the preview button again to view the image and note the new timestamp:</li>
</ul>
<p><img src="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/VIDEO%20ANALYTICS%20USING%20APACHE%20SPARK%20STREAMING/assets/VideoAnalytics20.jpeg?raw=true" alt="alt-tag"></p>
<p>P.S: Here face.jpeg is an generated image file which is created in a regular time interval and stored in DLP’s Hadoop platform.  The image gets updated at regular time intervals.</p>
<p>As an example we have used only small set of data for streaming in our application as a result of which different face images are getting generated and saved in the hadoop platform based on the timestamp. This process will continue till the stream data gets <a href="http://finished.In">finished.In</a> this example, we are viewing images at two different time intervals.</p>
<table class="table table-striped table-bordered">
<thead>
<tr>
<th>Input</th>
<th>Output</th>
</tr>
</thead>
<tbody>
<tr>
<td>The stream data from mockup video source from DLP.</td>
<td>Preview the image files from DLP Hadoop environment.</td>
</tr>
</tbody>
</table>
<h3><a id="Step_9__Stop_Eclipse_service_451"></a><strong>Step 9 : Stop Eclipse service</strong></h3>
<p>After viewing the images from DLP environment, we need to stop the Eclipse service from DLP.</p>
<p>Please refer screenshot below:</p>
<p><img src="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/VIDEO%20ANALYTICS%20USING%20APACHE%20SPARK%20STREAMING/assets/VideoAnalytics8.jpeg?raw=true" alt="alt-tag"></p>
<h2><a id="LESSONS_LEARNT_459"></a>LESSONS LEARNT</h2>
<ol>
<li>
<p>How to integrate Apache Spark Streaming with Kafka.</p>
</li>
<li>
<p>How to perform video analytic functions in a real-time stream application.</p>
</li>
<li>
<p>How to save a file to HDFS.</p>
</li>
</ol>
<h2><a id="Congratulations_You_have_successfully_completed_the_Learning_Lab_468"></a><strong>Congratulations! You have successfully completed the Learning Lab!</strong></h2>

</body></html>