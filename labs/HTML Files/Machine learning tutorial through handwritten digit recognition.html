<!DOCTYPE html><html><head><meta charset="utf-8"><title>Untitled Document.md</title><style></style></head><body id="preview">
<h1><a id="MACHINE_LEARNING_TUTORIAL_THROUGH_HANDWRITTEN_DIGIT_RECOGNITION_0"></a>MACHINE LEARNING TUTORIAL THROUGH HANDWRITTEN DIGIT RECOGNITION</h1>
<p><strong>Programming Language Used</strong>: Python</p>
<p><strong>Executed on</strong>: Hadoop Big Data Platform</p>
<p><strong>Visualization Tool Used</strong>: Apache Zeppelin</p>
<h2><a id="OVERVIEW_8"></a>OVERVIEW</h2>
<p>This learning lab can be used as a guide to get a high level understanding to use machine learning in order to let data drive decision making. We will use a dataset of tens of thousands of images of handwritten numbers, allowing the models to learn how to decide what digit each image represents. This is a supervised classification model, since we are trying to group the images into any of the digits 0 through 9 and we have labels for those digits. This task, like many machine learning and artificial intelligence tasks, has powerful practical implications. Here we are using the MNIST dataset, which was collected by the United States Census Bureau. Many organizations, such as the US Post Office, are using machine learning algorithms currently in order to provide more accurate and efficient reading of handwritten addresses and zip codes.</p>
<p>We will be using DevNet Data Learning Platform referred as “DLP” during the course.</p>
<h2><a id="PREREQUISITES_14"></a>PRE-REQUISITES</h2>
<ol>
<li>
<p>Install Chrome Browser.</p>
</li>
<li>
<p>Basic understanding of Apache Hadoop and Big Data.</p>
</li>
<li>
<p>Basic knowledge of Python.</p>
</li>
<li>
<p>Obtain access to Data Learning Platform -<br>
Refer: <a href="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/HowToAccessDTLP.md">https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/WORD COUNT USING SCALA WITH APACHE SPARK/HowToAccessDTLP.md</a></p>
</li>
</ol>
<h2><a id="LEARNING_OBJECTIVES_26"></a>LEARNING OBJECTIVES</h2>
<ol>
<li>
<p>To get familiarized with the DLP (Data Learning Platform).</p>
</li>
<li>
<p>To get familiarized with writing and executing python code in Zeppelin.</p>
</li>
<li>
<p>To get a high level understanding to use machine learning.</p>
</li>
</ol>
<h2><a id="TERMINOLOGIES_USED_35"></a>TERMINOLOGIES USED</h2>
<h3><a id="APACHE_ZEPPELIN__AN_INTRODUCTION_37"></a>APACHE ZEPPELIN - AN INTRODUCTION</h3>
<p>Zeppelin is a web-based notebook that enables interactive data analytics. You can make beautiful data-driven, interactive and collaborative documents with SQL, Scala and more.</p>
<p>Zeppelin enables data engineers, data analysts and data scientists to be more productive by developing, organising, executing, and sharing data code and visualising results without referring to the command line or knowing the cluster details. It brings data exploration, visualization, sharing and collaboration features to Spark. It supports Python and also a growing list of programming languages such as Scala, Hive, SparkSQL, shell and markdown. The various languages are supported via Zeppelin language interpreters. Zeppelinâ€™s notebooks provides interactive snippet-at-time experience to data scientist.</p>
<h6><a id="Key_Features_43"></a>Key Features:</h6>
<ol>
<li>
<p>Web based notebook style editor.</p>
</li>
<li>
<p>Built-in Apache Spark support.</p>
</li>
</ol>
<h6><a id="Use_Cases_for_Zeppelin__49"></a>Use Cases for Zeppelin :</h6>
<ol>
<li>
<p>Data Exploration and discovery</p>
</li>
<li>
<p>Data Visualization - Tables,graphs and charts</p>
</li>
<li>
<p>Interactive snippet-at-a-time experience</p>
</li>
<li>
<p>Collaboration and publishing</p>
</li>
</ol>
<p>For more details, please refer : <a href="https://zeppelin.apache.org/">https://zeppelin.apache.org/</a></p>
<h2><a id="PROCESS_OVERVIEW_62"></a>PROCESS OVERVIEW</h2>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/assets/images/Process3.jpeg?raw=true" alt="alt-tag"></p>
<p>Please follow the steps given below to launch the workspace and execute the program.</p>
<h3><a id="Step_1__Select_Learning_Lab_from_DLP_68"></a><strong>Step 1 : Select Learning Lab from DLP</strong></h3>
<p>After launching the DLP dashboard page, navigate to learning labs tab. Select the learning lab “Machine Learning tutorial through Handwritten Digit Recognition” and click on “Start” button as shown in screenshot below:</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning1.jpg?raw=true" alt="Figure"></p>
<h3><a id="Step_2__Workspace_Page_75"></a><strong>Step 2 : Workspace Page</strong></h3>
<p>On click of Start button, user will be navigated to a workspace page where all the components - IDE, Tools and Microservices required to execute the program are available. Please refer the screenshot below:</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning2.jpg?raw=true" alt="Figure"></p>
<h3><a id="Step_3__Start_Service_in_Workspace_81"></a><strong>Step 3 : Start Service in Workspace</strong></h3>
<p>Points to Note:</p>
<ul>
<li>Start the Zeppelin service.Please refer screenshot below:</li>
</ul>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning3.jpg?raw=true" alt="Figure"></p>
<ul>
<li>Once started, the colour of the icon changes to green and launch button is enabled.</li>
</ul>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning4.jpg?raw=true" alt="Figure"></p>
<ul>
<li>If the task is in stopped status then click on it to start again.</li>
</ul>
<h3><a id="Step_4__Launch_Zeppelin_95"></a><strong>Step 4 : Launch Zeppelin</strong></h3>
<p>Click on launch button and user will be navigated to Zeppelin home page.</p>
<p>Points to note:</p>
<p>(The pop-up may be blocked in browser configuration. Click on the red pop up blocker icon and select **Always allow pop-up from <a href="http://xxx.xxx.xxx.xxx">http://xxx.xxx.xxx.xxx</a> ** and click on <strong>Done</strong> button.)</p>
<p><img src="https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/DATA%20EXPLORATION%2C%20ANALYSIS%20AND%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/popUpBlockerAllowed.PNG?raw=true" alt="Figure"></p>
<p>On enabling the pop-up, Zeppelin would open in a new window taking you to Zeppelin home page with pre-configured notebook - “Cisco DevNet DLP Machine Learning” as shown in screenshot below.</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning5.jpg?raw=true" alt="Figure"></p>
<h3><a id="Step_5__Select_Zeppelin_notebook_110"></a><strong>Step 5 : Select Zeppelin notebook</strong></h3>
<ul>
<li>Select pre-created notebook - “Cisco DevNet DLP Machine Learning” from home page and below mentioned screen would be shown to the you:</li>
</ul>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning6.jpg?raw=true" alt="Figure"></p>
<ul>
<li>The Zeppelin interactive,development and visualisation environment is now ready for use. The notebook - “zeppelin-data-visualization-learning-labs” has all the codes required for executing this learning lab. Each of the code shown below needs to be executed sequentially.</li>
</ul>
<h3><a id="Step_6__Prepare_for_Modeling_119"></a><strong>Step 6 : Prepare for Modeling</strong></h3>
<p><strong>Access and Format data</strong></p>
<pre><code class="language-json">%python

### Below is a function to read in the image and label data.

### Inspired by the following gist: https://gist.github.com/akesling/5358964
import os
import struct
import numpy as np
import re
import random
from __future__ import division

def read(dataset = "training", path = "."):
    """
    Python function for importing the MNIST data set.  It returns an iterator
    of 2-tuples with the first element being the label and the second element
    being a numpy.uint8 2D array of pixel data for the given image.
    """
    if dataset == "training":
        img_file = os.path.join(path, 'train-images-idx3-ubyte')
        lbl_file = os.path.join(path, 'train-labels-idx1-ubyte')
    elif dataset == "testing":
        img_file = os.path.join(path, 't10k-images-idx3-ubyte')
        lbl_file = os.path.join(path, 't10k-labels-idx1-ubyte')
    else:
        raise ValueError("dataset must be 'testing' or 'training'")
    # Load everything in some numpy arrays
    with open(lbl_file, 'rb') as lbl_fc:
        (magic, num) = struct.unpack("&gt;II", lbl_fc.read(8))
        lbl = np.fromfile(lbl_fc, dtype=np.int8)
    with open(img_file, 'rb') as img_fc:
        (magic, num, rows, cols) = struct.unpack("&gt;IIII", img_fc.read(16))
        img = np.fromfile(img_fc, dtype=np.uint8).reshape(len(lbl), rows, cols)
    # If we wanted an iterator which returns each image in turn
    # get_img = lambda idx: (lbl[idx], img[idx])
    #    for i in xrange(len(lbl)):
    #        yield get_img(i)
    return (lbl, img)


### Here is a function to display images
def show(image, lbl=None, prd=None, idx=None):
    """
    Render a given numpy.uint8 2D array of pixel data.
    """
    from matplotlib import pyplot
    import matplotlib as mpl
    plt_title = ""
    fig = pyplot.figure()
    ax = fig.add_subplot(1,1,1)
    imgplot = ax.imshow(image, cmap=mpl.cm.Greys)
    imgplot.set_interpolation('nearest')
    ax.xaxis.set_ticks_position('top')
    ax.yaxis.set_ticks_position('left')
    if lbl is not None:
        plt_title += "Label: %d" % lbl
    if prd is not None:
        if len(plt_title) &gt; 0:
            plt_title += "; Prediction: %d" % prd
        else:
            plt_title += "Prediction: %d" % prd
    if idx is not None:
        if len(plt_title) &gt; 0:
            plt_title += "; Image Index: %d" % idx
        else:
            plt_title += "Image Index: %d" % idx
    if len(plt_title) &gt; 0:
        pyplot.title(plt_title, y=1.05)
    z.show(pyplot)
    pyplot.close()


def scale(array, method='zero-one'):
    """Scale the data so that it can work more easily with algorithms like SVM,
    which require scaling. For now, this only allows for scaling between the
    range of [0,1], inclusive.
    This assumes a numpy array is passed in.
    """
    if method != 'zero-one':
        raise NotImplementedError("Only 'zero-one' method currently implemented.")
    array = array - array.min()
    array = array / array.max()
    return array


def zepp_print_array(array, prefix="%table ", orig_sep=r"([-0-9\.]) +([-0-9])",
                     new_sep=r"\1\t\2", remove=r"[\[\]]", headers=None):
    """Prepare a string that is ready to be printed with Zeppelin's nice table
    formatting.
    """
    if len(array.shape) != 2:
        raise NotImplementedError("'zepp_print_array' can currently only print 2D arrays.")
    if headers is None:
        headers = list(range(array.shape[1]))
        headers = ['Digit ' + str(header) for header in headers]
    if isinstance(headers, list) or isinstance(headers, tuple):
        headers = "\t".join(headers)
    if not isinstance(headers, str):
        headers = str(headers)
    str_array = np.array_str(array) # Convert ndarray to a string
    #    sep_re = re.compile(orig_sep)
    str_array = re.sub(orig_sep, new_sep, str_array)
    str_array = re.sub(orig_sep, new_sep, str_array)
    str_array = re.sub(r' ', '', str_array)
    str_array = re.sub(remove, "", str_array)
    str_array = prefix + headers + '\n' + str_array
    print(str_array)


def gen_random(size):
    while True:
        yield random.randrange(size)


def gen_next(*args):
    if len(args) == 1:
        for next_val in range(args[0]):
            yield next_val
    elif len(args) == 2:
        for next_val in range(args[0], args[1]):
            yield next_val
    elif len(args) == 3:
        for next_val in range(args[0], args[1], args[2]):
            yield next_val


# First we will read in the data and get it in the format / shape that we'll need.
(lbls, orig_imgs) = read(dataset="testing", path="/tmp")


# Now flatten the imgs one dimension.  We'll use an algorithm that does not require local neighborhood knowledge, so
# each pixel is simply considered a feature.
imgs = orig_imgs.reshape(len(orig_imgs), -1)

# We also need to scale our image, since SVM is impacted by scale. The following call
# will scale all image values to [0,1].
imgs = scale(imgs)

# Now create some generators that we'll use to look through images.
random_num = gen_random(len(lbls))
next_num = gen_next(len(lbls))
</code></pre>
<p>Execute the code by clicking on the “Ready” icon highlighted as shown in screenshot below:.</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning7.jpg?raw=true" alt="Figure"></p>
<p>On successful execution of code, the icon changes to “Finished” as shown in screenshot below:</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning17.jpg?raw=true" alt="Figure"></p>
<h3><a id="Step_7__Look_at_Example_Image_275"></a><strong>Step 7 : Look at Example Image</strong></h3>
<p>Below we look at one of the images, just to get a sense of what we’re looking at. The paragraph below is designed to let you keep re-running it to look at different images.<br>
Theÿindexÿof the image is referring to which image number, of the 10,000 available, you are looking at.<br>
In this case, we will look at each image in order. (The first image, then the second, then third, etc.)</p>
<p>The code is as follows:</p>
<pre><code class="language-json">%python
next_idx = next(next_num)
show(orig_imgs[next_idx], lbls[next_idx], idx=next_idx)
</code></pre>
<p>Execute the code by clicking on the “Ready” icon highlighted as shown in screenshot below:.</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning8.jpg?raw=true" alt="Figure"></p>
<p>On successful execution of code, the icon changes to “Finished” as shown in screenshot below:</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning18.jpg?raw=true" alt="Figure"></p>
<h3><a id="Step_8__Look_at_Random_Example_Image_299"></a><strong>Step 8 : Look at Random Example Image</strong></h3>
<p>Below we again look at one of the images, but this time it is randomly chosen, to give a better sense of variety.<br>
Again, the code below can be re-run to examine different images (each randomly chosen).</p>
<pre><code class="language-json">%python
random_idx = next(random_num)
show(orig_imgs[random_idx], lbls[random_idx], idx=random_idx)
</code></pre>
<p>Execute the code by clicking on the “Ready” icon highlighted as shown in screenshot below:.</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning9.jpg?raw=true" alt="Figure"></p>
<p>On successful execution of code, the icon changes to “Finished” as shown in screenshot below:</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning19.jpg?raw=true" alt="Figure"></p>
<h3><a id="Step_9__Split_Training_and_Validation_Data_320"></a><strong>Step 9 : Split Training and Validation Data</strong></h3>
<p>Below we define the training and validation datasets. We define these by capturing an array of the indices for each set.<br>
Machine learning is data-driven modeling. As such, the data is the most precious part. We need to use the data both to train / create our model, as well as to validate the model.<br>
To prevent overfitting, as discussed in the lecture portion of this lab, we need to validate on an entirely different data set than that we use to train.</p>
<pre><code class="language-json">%python
# Split the data into training and test data sets.
train_set_size = int(len(lbls) * 0.5)
train_idx = list(range(train_set_size))
test_idx = list(range(train_set_size, len(lbls)))
</code></pre>
<p>Execute the code by clicking on the “Ready” icon highlighted as shown in screenshot below:.</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning10.jpg?raw=true" alt="Figure"></p>
<p>On successful execution of code, the icon changes to “Finished” as shown in screenshot below:</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning20.jpg?raw=true" alt="Figure"></p>
<h3><a id="Step_10__Modeling_345"></a><strong>Step 10 : Modeling</strong></h3>
<p>Below the modeling begins. We are using support vector machine(SVM) models. We are performing a supervised classification model. The model is supervised because we have a specified goal, and training data which provide answers to that goal. Specifically, we have handwritten digits which are to be recognized as particular digits, and we have the answers to those digits, as shown in the image above.<br>
The model is classifying because there are particular, discrete bins that we are trying to fit the predictions into. Here we are trying to tell to which of the numbers 0 - 9 the handwritten digits look most similar. This might seem like regression, since we are talking about numbers, but it is classification, because each number looks drastically different from its neighbor. The number ‘1’ might seem similar to ‘0’ or ‘2’ in our minds, but as it is written, they look nothing alike. In fact, ‘0’ looks most similar to ‘8’, and maybe ‘1’ looks most similar to ‘7’. We’ll see, in the validation, that in fact the most frequent mix-ups are with digits that look most similar.</p>
<p>Find Model Parameters</p>
<p>We will first run several different support vector machine(SVM) models, changing the parameters to find which work best. Specifically, we look at a polynomial kernel versus a radial kernel, as well as several different values for the penalty parameter, ‘C’.When modeling, we only use the training data. The remaining data we will use for final validation for assessing the model quality at the end.<br>
Necessarily, this work also generates the model as well.</p>
<p>The modeling performs a 3-fold cross-validation, further breaking the training dataset into 2 groups, one of which is used for model cross-validation. This is different from the final validation mentioned immediately above.<br>
The code below takes a long time to run, potentially up to 10 minutes.</p>
<pre><code class="language-json">%python
# Here we will optimize the "hyper-parameters" of our model, *if needed*.  Optimizing these
# parameters takes a long time, so this should only be run when needed.
# We will use a Support Vector Machine (SVM) algorithm.

from sklearn.model_selection import GridSearchCV # This is the newer library, might be needed instead.
from sklearn import svm, metrics
#from sklearn.grid_search import GridSearchCV

parameters = {'kernel':('poly', 'rbf'), 'C':[1, 10, 100]}
estimator = svm.SVC()
classifier = GridSearchCV(estimator, parameters)
classifier.fit(imgs[[train_idx]], lbls[[train_idx]])

GridSearchCV(cv=None, error_score='raise',
estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
max_iter=-1, probability=False, random_state=None, shrinking=True,
tol=0.001, verbose=False),
fit_params={}, iid=True, n_jobs=1,
param_grid={'kernel': ('poly', 'rbf'), 'C': [1, 10, 100]},
pre_dispatch='2*n_jobs', refit=True, return_train_score=True,
scoring=None, verbose=0)
</code></pre>
<p>Execute the code by clicking on the “Ready” icon highlighted as shown in screenshot below:.</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning11.jpg?raw=true" alt="Figure"></p>
<p>On successful execution of code, the icon changes to “Finished” as shown in screenshot below:</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning21.jpg?raw=true" alt="Figure"></p>
<h3><a id="Step_11__Assess_Model_Quality_394"></a><strong>Step 11 : Assess Model Quality</strong></h3>
<p>The model has been completed. In the next few sections, we’ll assess the model both by rigorous mathematical calculations and assessments, as well as studying a sampling of what went wrong.<br>
Examine ?Confusion Matrix?<br>
We will first look at the confusion matrix, a fancy phrase for a fairly simple concept.</p>
<p>Along the columns, we have the count of images with that predicted digit, and across the rows, we have the count for the actual digit. For instance, the upper-left corner represents the number of images that were labelled as 0’s, and that were successfully predicted to be zeros. The upper-right corner represents the number of images that were labelled as 0’s and that were incorrectly predicted to be 9’s.</p>
<p>Therefore, the diagonal from upper-left to lower-right represents all of the digits that were predicted properly, or which the machine learning algorithm properly modelled. Anything not on the diagonal was improperly modeled.</p>
<p>This code below, again, can potentially take up to 10 minutes.</p>
<pre><code class="language-json">%python
from sklearn import metrics
predicted = classifier.predict(imgs)
print(metrics.confusion_matrix(lbls[[test_idx]], predicted[[test_idx]]))

[[509 0 0 1 2 0 2 3 3 0]
[ 0 552 0 0 0 1 1 0 10 0]
[ 8 0 453 1 19 2 4 3 9 3]
[ 2 0 20 443 1 8 0 4 28 4]
[ 1 0 0 0 471 0 2 2 2 4]
[ 2 4 5 12 17 377 12 1 6 0]
[ 0 0 6 0 5 0 485 0 0 0]
[ 2 2 5 0 2 0 0 481 0 24]
[ 0 4 3 0 3 8 7 4 455 1]
[ 2 0 0 0 30 11 0 7 5 434]]
</code></pre>
<p>Execute the code by clicking on the “Ready” icon highlighted as shown in screenshot below:.</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning12.png?raw=true" alt="Figure"></p>
<p>On successful execution of code, the icon changes to “Finished” as shown in screenshot below:</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning22.jpg?raw=true" alt="Figure"></p>
<h3><a id="Step_12__Plot_cosine_wave_431"></a><strong>Step 12 : Plot cosine wave</strong></h3>
<p>The Classification Report<br>
There are several other methods to validate the quality of the model. The method given below is very powerful, but as we delve further into these mathematical methods, they become more complex. We won’t delve into all of it, but we will touch upon the precision and recall.</p>
<p>Wikipedia has a nice discussion on the precision and recall, given below. In short, the precision is the percent of things that you predicted that were correctly predicted. E.g., if 500 images were predicted to be 0’s, and 490 of them were actually labelled as zeros, then the precision would be 98% (490 / 500).<br>
The recall is the percent of things that truly were a value, that you correctly predicted to be that value. E.g., if 1000 images were labelled as 0’s, and 950 of them were predicted as zeros, then the precision would be 95%.</p>
<pre><code class="language-json">READY
%python
print(metrics.classification_report(lbls[[test_idx]], predicted[[test_idx]]))
precision recall f1-score support
0 0.97 0.98 0.97 520
1 0.98 0.98 0.98 564
2 0.92 0.90 0.91 502
3 0.97 0.87 0.92 510
4 0.86 0.98 0.91 482
5 0.93 0.86 0.89 436
6 0.95 0.98 0.96 496
7 0.95 0.93 0.94 516
8 0.88 0.94 0.91 485
9 0.92 0.89 0.91 489
avg / total 0.93 0.93 0.93 5000

</code></pre>
<p>Execute the code by clicking on the “Ready” icon highlighted as shown in screenshot below:.</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning13.jpeg?raw=true" alt="Figure"></p>
<p>On successful execution of code, the icon changes to “Finished” as shown in screenshot below:</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning23.jpg?raw=true" alt="Figure"></p>
<h3><a id="Step_13__Model_Overfitting_464"></a><strong>Step 13 : Model Overfitting</strong></h3>
<p>In the lesson slides, we had discussed model overfitting. In the work here, we addressed model overfitting in two ways:</p>
<ol>
<li>We internally assessed the SVM model quality via a 3-fold cross-validation</li>
<li>When we looked at the performance of the model, we only compared the predictive capabilities of the validation dataset, the dataset that was never used for any modeling in any way.<br>
Let’s break that second rule above, and instead evaluate the performance by looking at how well the model works in predicting the training dataset. This will lead us to overconfidence of the quality of our models, when evaluating, and when internally assessing, it could also allow for overfitting.</li>
</ol>
<pre><code class="language-json">%python

#print(metrics.confusion_matrix(lbls[[train_idx]], predicted[[train_idx]]))
print(metrics.classification_report(lbls[[train_idx]], predicted[[train_idx]]))
precision recall f1-score support
0 0.97 0.98 0.98 460
1 0.97 0.99 0.98 571
2 0.97 0.95 0.96 530
3 0.95 0.95 0.95 500
4 0.93 0.97 0.95 500
5 0.95 0.95 0.95 456
6 0.97 0.97 0.97 462
7 0.95 0.94 0.94 512
8 0.95 0.94 0.95 489
9 0.95 0.92 0.93 520
avg / total 0.96 0.96 0.96 5000
</code></pre>
<p>Execute the code by clicking on the “Ready” icon highlighted as shown in screenshot below:.</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning14.jpg?raw=true" alt="Figure"></p>
<p>On successful execution of code, the icon changes to “Finished” as shown in screenshot below:</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning24.jpg?raw=true" alt="Figure"></p>
<p>Overall the precision and recall values are much closer to 100%. But this does not represent the truth. We will never want to know how well a model can predict data we have already fed it, but instead future data that is being seen for the first time by the model.<br>
Delving into Misclassified Images<br>
Finally, let’s look at some of those images that were misclassified (within the test set).<br>
It’s often worthwhile to look at the images that were misclassified, to better understand what might have gone wrong. Were the misclassifications because some of the handwritten digits were just really hard to read? Were they misclassified because there is some systematic mistake happening, such as 2’s and 3’s consistently being mistaken? Sometimes it helps simply to drive down into some of the errors.</p>
<p>First we find the indices of the images that were misclassified.</p>
<pre><code class="language-json">%python

mislbld_idx = [idx for expct, pred, idx in zip(lbls[[test_idx]], predicted[[test_idx]], test_idx) if expct != pred]
img_gen = gen_next(len(mislbld_idx))
</code></pre>
<p>Execute the code by clicking on the “Ready” icon highlighted as shown in screenshot below:.</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning15.jpg?raw=true" alt="Figure"></p>
<p>On successful execution of code, the icon changes to “Finished” as shown in screenshot below:</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning25.jpg?raw=true" alt="Figure"></p>
<p>Then we show a misclassified image, in a manner similar to what we’d done earlier, where we can keep re-running the code below in order to see the next misclassified image.</p>
<pre><code class="language-json">%python

next_img = mislbld_idx[next(img_gen)]
show(orig_imgs[next_img], lbls[next_img], predicted[next_img], idx=next_img)
</code></pre>
<p>Execute the code by clicking on the “Ready” icon highlighted as shown in screenshot below:.</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning16.jpg?raw=true" alt="Figure"></p>
<p>On successful execution of code, the icon changes to “Finished” as shown in screenshot below:</p>
<p><img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning26.jpg?raw=true" alt="Figure"></p>
<h2><a id="LESSONS_LEARNT_536"></a>LESSONS LEARNT</h2>
<p>In this lesson we walked through machine learning modeling, where we predicted handwritten digits, automatically assigning a value to the handwritten digit. This sort of task is not only useful, but already being used within the industry, such as by the post office to better deliver mail to its customers.</p>
<p>This work involved:</p>
<ol>
<li>Some preparatory work that we did not delve deeply into, such as reading the data.</li>
<li>Visually looking at our digit images, so that we can better understand the task at hand.</li>
<li>Creating a training and testing dataset.</li>
<li>Generating the model.This was done either with a wider search for model parameters, or using preset parameters.</li>
<li>Predicting digits based upon the test dataset images.</li>
<li>Examining the quality of those predictions in multiple ways including a slight divergence to show an inaccurate means of    assessing quality by using the training dataset.</li>
<li>Finally delving into some of the misclassified images to understand what may have caused the problems.</li>
</ol>
<h1><a id="Congratulations_You_have_successfully_completed_the_Learning_Lab_551"></a><strong>Congratulations! You have successfully completed the Learning Lab!</strong></h1>

</body></html>