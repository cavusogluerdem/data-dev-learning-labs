# **CLIMATE NETWORK DATA ANALYSIS FOR STORM TRACKING WITH SPARK SQL AND DATAFRAMES**

**Programming Language Used** : Scala

**Executed on** : Hadoop Big Data Platform

**Visualization Tool Used** : Apache Zeppelin

## OVERVIEW

This learning lab can be used as a guide to get a high level understanding of using Scala, Spark SQL and Dataframes with Apache Zeppelin for storm tracking with the available climate network data. It describes how to use Scala functions and write interactive code from Zeppelin. It also gives a description to get familiar with the concepts and usage of Spark SQL and Spark Dataframes. 

We will be using DevNet Data Learning Platform referred as "DLP" during the course. 

## PRE-REQUISITES

1.	Install Chrome Browser.

2.	Basic understanding of Hadoop Distributed File System (HDFS).

3.  Basic knowledge of SQL (Structured Query Language).

4.  Basic knowledge of Scala programming language.

5.  Obtain access to Data Learning Platform -
                                                          Refer: https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/HowToAccessDTLP.md


## LEARNING OBJECTIVES

1. To get familiarized with the DLP (Data Learning Platform).

2. To get familiarized with writing and executing Scala code in Zeppelin. 

3. To get familiarized with the usage of Apache Zeppelin to visualize data.

4. Get familiarized with the concepts and usage of SparkSQL.

5. Get familiarized with the concepts and usage of Spark DataFrames.


## TERMINOLOGIES USED

### APACHE ZEPPELIN - AN INTRODUCTION

**Zeppelin is a web-based notebook that enables interactive data analytics.** You can make beautiful data-driven, interactive and collaborative documents with SQL, Scala and more. 

Zeppelin enables data engineers, data analysts and data scientists to be more productive by developing, organising, executing, and sharing data code and visualising results without referring to the command line or knowing the cluster details. It brings data exploration, visualization, sharing and collaboration features to Spark. It supports Python and also a growing list of programming languages such as Scala, Hive, SparkSQL, shell and markdown. The various languages are supported via Zeppelin language interpreters. Zeppelin’s notebooks provides interactive snippet-at-time experience to data scientist.

###### Key Features:

1. Web based notebook style editor.

2. Built-in Apache Spark support.

###### Use Cases for Zeppelin :

1. Data Exploration and discovery

2. Data Visualization - Tables,graphs and charts

3. Interactive snippet-at-a-time experience

4. Collaboration and publishing

For more details, please refer : https://zeppelin.apache.org/

### SCALA - AN INTRODUCTION

**Scala is an acronym for “Scalable Language”.** This means that Scala grows with you. Scala could be written by typing one-line expressions and observing the results and could also be used for large mission critical systems. Scala could also be considered as a scripting language and is a pure-bred object-oriented language. The language supports advanced component architectures through classes and traits. Even though its syntax is fairly conventional, Scala is also a full-blown functional language. Scala runs on the JVM. Java and Scala classes can be freely mixed, no matter whether they reside in different projects or in the same. Scala makes deliver things faster with less code.

For more details, please refer: http://www.scala-lang.org/what-is-scala.html https://en.wikipedia.org/wiki/Scala_(programming_language)

### HADOOP - AN INTRODUCTION

**Apache Hadoop is an open-source software framework for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware.** 

To understand Hadoop, there are two fundamental things about it - How Hadoop stores files and how it processes data. The framework that is used in Hadoop to process data is called MapReduce.

All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common and should be automatically handled by the framework. The core of Apache Hadoop consists of a storage part, known as Hadoop Distributed File System (HDFS), and a processing part called MapReduce. Hadoop splits files into large blocks and distributes them across nodes in a cluster.

Example : Imagine a file that is larger than the capacity of a computer then it would not be possible to store that file. Hadoop allows to store files bigger than what can be stored on one particular node or server. So it provides an ability to store very, very large files and also lets to store many, many files.

For more details, please refer: (https://en.wikipedia.org/wiki/Apache_Hadoop)

### SPARK SQL AND DATAFRAMES - AN INTRODUCTION 

**Spark SQL is a Spark module for structured data processing.** 

Spark SQL have these important features:

* Integrated - Spark SQL lets you query structured data inside Spark programs, using either SQL or a familiar DataFrame API. Usable in Java, Scala, Python and R.
* Uniform Data Access - DataFrames and SQL provide a common way to access a variety of data sources, including Hive, Avro, Parquet, ORC, JSON, and JDBC. You can even join data across these sources.
* Hive Integration - Spark SQL supports the HiveQL syntax as well as Hive SerDes and UDFs, allowing you to access existing Hive warehouses.
* Standard Connectivity - A server mode provides industry standard JDBC and ODBC connectivity for business intelligence tools.

**A DataFrame is a Dataset organized into named columns.**

It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset to represent a DataFrame.

Basic steps to use Spark SQL and DataFrame:

1) Create a DataFrame from an existing RDD, from a Hive table or JSON/CSV files etc.
2) Register the DataFrame as a table.
3) Execute sql operations on the created table.

For more details, please refer:
https://spark.apache.org/docs/latest/sql-programming-guide.html

### GLOBAL HISTORICAL CLIMATE NETWORK (GHCN) - Daily Data Set

**GHCN-Daily** - An integrated database of daily climate summaries from land surface stations across the globe. GHCN-Daily is comprised of daily climate records from numerous sources that have been integrated and subjected to a common suite of quality assurance reviews.
It contains records from over 100,000 stations in 180 countries and territories. NCEI provides numerous daily variables, including maximum and minimum temperature, total daily precipitation, snowfall, and snow depth; however, about one half of the stations report precipitation only. Both the record length and period of record vary by station and cover intervals ranging from less than a year to more than 175 years.

In this learning lab, we will use the observations from global stations in 2017. The details are stored in two files viz. 
1. 2017.csv 
2. ghcnd-stations.txt

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/FileInformation.jpeg)


## PROCESS OVERVIEW 

![alt-tag](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/assets/images/Process16.jpeg?raw=true)

Please follow the steps given below to launch the workspace and execute the lab.

### **Step 1 : Select Learning Lab from DLP**

After launching the DLP dashboard page, navigate to learning labs tab. Select the learning lab "Climate Network Data Analysis for Storm tracking with Spark SQL and Dataframes" and click on "Start" button as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateData1.jpeg)

### **Step 2 : Workspace Page**

On click of Start button, user will be navigated to a workspace page where all the components - IDE, Tools and Microservices required to execute the program are available. Please refer the screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateData2.jpeg)

### **Step 3 : Start Zeppelin and Map services in Workspace**

Points to Note:

* Start the Zeppelin service.Please refer screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateData2.jpeg)

* Once started, the colour of the icon changes to green and launch button is enabled.

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateData3.jpeg)

* If the task is in stopped status then click on it to start again. 

### **Step 4 : Launch Zeppelin**

Click on launch button and user will be navigated to Zeppelin home page.

Points to note:

(The pop-up may be blocked in browser configuration. Click on the red pop up blocker icon and select **Always allow pop-up from http://xxx.xxx.xxx.xxx ** and click on **Done** button.)  

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/DATA%20EXPLORATION%2C%20ANALYSIS%20AND%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/popUpBlockerAllowed1.jpeg?raw=true)

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/DATA%20EXPLORATION%2C%20ANALYSIS%20AND%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/popUpBlockerAllowed2.jpeg?raw=true)

On enabling the pop-up, Zeppelin would open in a new window taking the user to Zeppelin home page as shown in screenshot below:
![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateData4.jpeg)


### **Step 5 : Select Zeppelin notebook**

* Select pre-created notebook - "open data - GHCN data analysis" from home page as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateData6.jpeg)

* The Zeppelin interactive,development and visualisation environment is now ready for use. The pre-created notebook has all the codes required for executing this learning lab. Each of the code shown below needs to be executed sequentially. 

### **Step 6 : Execute Scala Code**

We are using code written in Scala for data generation. 





Following functions are being done by the Scala Code :

* Login to DLP and navigate to the workspace.
* Check the input files -2017.csv and ghcnd-stations.txt
* Start services – Zeppelin and Map View
* Launch Zeppelin and select the pre-created notebook
* Load Spark package

The code is as follows: 
```json
      %dep
      z.reset()
      z.addRepo("Spark Packages Repo").url("http://dl.bintray.com/spark-packages/maven")
      z.load("com.databricks:spark-csv_2.10:1.2.0")
```
Execute the code by clicking on the "Ready" icon highlighted as shown in screenshot below:. 

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateDataXXX.jpeg)

 On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateData7.jpeg)

* Create dataframes using the input files and register tables (Observations and Stations)
  The steps shown below explains the following:
  1. How to create dataframes.
  2. How to load the input files in to the created dataframes.
  3. How to register the created dataframes as temporary tables.
  
  The code is as follows:
```json
%spark

//define column headers for the observation values in file "2017.csv"

val column_headers = Seq("ID", "DATE", "ELEMENT", "DATA_VALUE", "M_FLAG", "Q_FLAG", "S_FLAG", "OBS_TIME")

//Load the "2017.csv" file into a dataframe
val observ_df = sqlContext
    .read
    .format("com.databricks.spark.csv")
    .option("header", "false")     // Use first line of all files as header
    .option("inferSchema", "true") // Automatically infer data types
    .load(sys.env("INPUT1_HDFS"))
    .toDF(column_headers: _*)      //load file , set columns


//define station class.
case class Station(ID: String, LATITUDE: String, LONGITUDE: String, ELEVATION: String, STATE: String)

//Load the "ghcnd-stations.txt" into a dataframe
val station_df = sc
    .textFile(sys.env("INPUT0_HDFS"))
    .map(s => Station(
        s.slice(0,11).trim,
        s.slice(12,20).trim,
        s.slice(21,30).trim,
        s.slice(31,37).trim,
        s.slice(38, 40).trim))
    .toDF()
                                 
//register the dataFrame  'observ_df' as a temporary table using the name 'observations'.
observ_df.registerTempTable("observations")

//register the dataFrame  'station_df' as a temporary table using the name 'stations'.
station_df.registerTempTable("stations")
```
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateData8.jpeg)


* Get the record count from created table “observations”
The code shown is to get the count of records from the created table -“observations”. The table holds the data from input file - “2017.csv”

```json
%sql
SELECT COUNT(1) as counts FROM observations
```
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateData9.jpeg)

* Query table Observations for a specific date (2017.01.21) and create a new table with the results - observations_20170121

The code shown is to get the data from the created table -"observations" as on a given date. The table holds the data from input file - "2017.csv".
The steps shown below explains the following:
- How to select the values from the table as on a given date to a dataframe.
- How to register the created dataframe as a temporary table.

```json
%spark
//select the obervation values as on 2017.01.01
val first_day_df = sqlContext.sql("SELECT observations.ID, observations.ELEMENT, observations.DATA_VALUE FROM observations WHERE observations.DATE=\"20170121\" ")
//register the data frame 'first_day_df' as a temporary table
first_day_df.registerTempTable("observations_20170121")
```
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateData10.jpeg)

* Validate the record count in the created table
The code shown is to validate the count of records from the created table -“observations_20170101”. The table holds the data from the table “observations” as on date 2017.01.21.
The steps shown below explains the following:
How to get the record count.
```json
%sql
SELECT COUNT(1) as counts FROM observations_20170121
```
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateData11.jpeg)

* Validate data from created table before performing Join operation
The steps shown below explains the following:
How to get the data from the created table.

```json
%sql
SELECT * FROM observations_20170121 limit 100
```
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateData12.jpeg)

* Perform Join operation (observations_20170121 and Stations)

The steps shown below explains the following:
How to join two tables and create a resultant dataframe.
How to register the created dataframe as a table.

```json
%spark

//join two tables
val joined_df = sqlContext.sql("""SELECT observations_20170121.ELEMENT, observations_20170121.DATA_VALUE, observations_20170121.ID, LATITUDE, LONGITUDE 
                                                    FROM observations_20170121
                                                    LEFT JOIN stations 
                                                    ON observations_20170121.ID = stations.ID """)
//register the data frame 'joined_df' as a temporary table
joined_df.registerTempTable("joined_table")
```
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateData13.jpeg)

* Query and Select core elements from the joined table. Perform further select operations.
The steps shown below explains the following:
How to get data from the joined table. (This table is created by joining two tables - observations_20170121 and stations as explained in previous steps)

```json
%sql
SELECT * FROM joined_table limit 20
```
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateData14.jpeg)

* Select precipitation and average wind speed from joined table
The steps shown below explains the following:
How to get specific elements from the newly created table. (This table is created by joining two table as explained in previous steps)

```json
%spark

//select the different elements from 'joined_table'
val prcp_df = sqlContext.sql("select ID, DATA_VALUE as PRCP from joined_table where ELEMENT=\"PRCP\" ")
val awnd_df = sqlContext.sql("select ID, DATA_VALUE as AWND from joined_table where ELEMENT=\"AWND\" ")

//register the data frame of different elements as temporary table
prcp_df.registerTempTable("prcp")
awnd_df.registerTempTable("awnd")
```
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateData15.jpeg)

* Analyze the storm distribution based on obtained values of wind speed and precipitation
```json
%spark

import org.apache.spark.sql.functions._

//Threshold for separate the weather into 4 category, heavy storm, middile storm, light storm, no storm
val precipitation_threshold_0 = 80
val precipitation_threshold_1 = 120
val precipitation_threshold_2 = 240

val wind_speed_threshold_0 = 50
val wind_speed_threshold_1 = 120
val wind_speed_threshold_2 = 250

//udf function for storm level decision.
val level_decision_func = udf( (a:Int, b:Int) => {
                                     if (a >= precipitation_threshold_0 && a < precipitation_threshold_1  && b >= wind_speed_threshold_0 && b < wind_speed_threshold_1 ){
                                     1   
                                    } else if (a >= precipitation_threshold_1 && a < precipitation_threshold_2  && b >= wind_speed_threshold_1 && b < wind_speed_threshold_2){
                                     2 
                                    } else if (a >= precipitation_threshold_2 && b >= wind_speed_threshold_2){
                                     3
                                    } else {
                                     0
                                    }
                                })
//Add a column "STORM_LEVEL" to the data frame
val level_df = prcp_wind_filter_df.withColumn("STORM_LEVEL", level_decision_func(prcp_wind_filter_df.col("Precipitation"), prcp_wind_filter_df.col("Average_Wind_Speed")) )

//Select 4 columns from the data frame "levle_df"
val result_df = level_df.select("LATITUDE", "LONGITUDE", "STORM_LEVEL", "Precipitation")
```
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateData16.jpeg)

* Save the result to HDFS(Preview output on DLP)
The steps shown below explains the following:
Check if the file already exists in HDFS. If yes, delete the same and create a new file.
Save the result as a .csv file.
```json
```
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateData17.jpeg)

* Visualize data using Map service
The final step is to visualize the storm level and precipitation using the Map service.

### **Step 7 : Geographical data visualization using Map service**
Navigate back to the platform and launch the map service by clicking on the launch button. The map opens up in a separate tab as shown below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateData18.jpeg)

It displays the various storm levels and precipitation.
The legend is as follows:

Storm Level:


Precipitation:



### **Step 8 : Stop Zeppelin and Map services in Workspace**

After viewing the details on Map from DLP environment, we need to stop the services from DLP.

Please refer screenshot below:
![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/ClimateData19.jpeg)

## LESSONS LEARNT

1. How to write and execute Scala code in Zeppelin.

2. How to use Apache Zeppelin to visualize data from two different tables.

3. How to use Spark SQL.

4. How to use Spark Dataframe.


# **Congratulations! You have successfully completed the Learning Lab!**
