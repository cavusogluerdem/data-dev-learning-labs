# **CLIMATE NETWORK DATA ANALYSIS FOR STORM TRACKING WITH SPARK SQL AND DATAFRAMES**

**Programming Language Used** : Scala

**Executed on** : Hadoop Big Data Platform

**Visualization Tool Used** : Apache Zeppelin

## OVERVIEW

This learning lab can be used as a guide to get a high level understanding of using Scala, Spark SQL and Dataframes with Apache Zeppelin for storm tracking with the available climate network data. It describes how to use Scala functions and write interactive code from Zeppelin. It also gives a description to get familiar with the concepts and usage of Spark SQL and Spark Dataframes. 

We will be using DevNet Data Learning Platform referred as "DLP" during the course. 

## PRE-REQUISITES

1.	Install Chrome Browser.

2.	Basic understanding of Hadoop Distributed File System (HDFS).

3.  Basic knowledge of SQL (Structured Query Language).

4.  Basic knowledge of Scala programming language.

5.  Obtain access to Data Learning Platform -
                                                          Refer: https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/HowToAccessDTLP.md


## LEARNING OBJECTIVES

1. To get familiarized with the DLP (Data Learning Platform).

2. To get familiarized with writing and executing Scala code in Zeppelin. 

3. To get familiarized with the usage of Apache Zeppelin to visualize data.

4. Get familiarized with the concepts and usage of SparkSQL.

5. Get familiarized with the concepts and usage of Spark DataFrames.


## TERMINOLOGIES USED

### APACHE ZEPPELIN - AN INTRODUCTION

**Zeppelin is a web-based notebook that enables interactive data analytics.** You can make beautiful data-driven, interactive and collaborative documents with SQL, Scala and more. 

Zeppelin enables data engineers, data analysts and data scientists to be more productive by developing, organising, executing, and sharing data code and visualising results without referring to the command line or knowing the cluster details. It brings data exploration, visualization, sharing and collaboration features to Spark. It supports Python and also a growing list of programming languages such as Scala, Hive, SparkSQL, shell and markdown. The various languages are supported via Zeppelin language interpreters. Zeppelin’s notebooks provides interactive snippet-at-time experience to data scientist.

###### Key Features:

1. Web based notebook style editor.

2. Built-in Apache Spark support.

###### Use Cases for Zeppelin :

1. Data Exploration and discovery

2. Data Visualization - Tables,graphs and charts

3. Interactive snippet-at-a-time experience

4. Collaboration and publishing

For more details, please refer : https://zeppelin.apache.org/

### SCALA - AN INTRODUCTION

**Scala is an acronym for “Scalable Language”.** This means that Scala grows with you. Scala could be written by typing one-line expressions and observing the results and could also be used for large mission critical systems. Scala could also be considered as a scripting language and is a pure-bred object-oriented language. The language supports advanced component architectures through classes and traits. Even though its syntax is fairly conventional, Scala is also a full-blown functional language. Scala runs on the JVM. Java and Scala classes can be freely mixed, no matter whether they reside in different projects or in the same. Scala makes deliver things faster with less code.

For more details, please refer: http://www.scala-lang.org/what-is-scala.html https://en.wikipedia.org/wiki/Scala_(programming_language)

### HADOOP - AN INTRODUCTION

**Apache Hadoop is an open-source software framework for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware.** 

To understand Hadoop, there are two fundamental things about it - How Hadoop stores files and how it processes data. The framework that is used in Hadoop to process data is called MapReduce.

All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common and should be automatically handled by the framework. The core of Apache Hadoop consists of a storage part, known as Hadoop Distributed File System (HDFS), and a processing part called MapReduce. Hadoop splits files into large blocks and distributes them across nodes in a cluster.

Example : Imagine a file that is larger than the capacity of a computer then it would not be possible to store that file. Hadoop allows to store files bigger than what can be stored on one particular node or server. So it provides an ability to store very, very large files and also lets to store many, many files.

For more details, please refer: (https://en.wikipedia.org/wiki/Apache_Hadoop)

### SPARK SQL AND DATAFRAMES - AN INTRODUCTION 

**Spark SQL is a Spark module for structured data processing.** 

Spark SQL have these important features:

* Integrated - Spark SQL lets you query structured data inside Spark programs, using either SQL or a familiar DataFrame API. Usable in Java, Scala, Python and R.
* Uniform Data Access - DataFrames and SQL provide a common way to access a variety of data sources, including Hive, Avro, Parquet, ORC, JSON, and JDBC. You can even join data across these sources.
* Hive Integration - Spark SQL supports the HiveQL syntax as well as Hive SerDes and UDFs, allowing you to access existing Hive warehouses.
* Standard Connectivity - A server mode provides industry standard JDBC and ODBC connectivity for business intelligence tools.

**A DataFrame is a Dataset organized into named columns.**

It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset to represent a DataFrame.

Basic steps to use Spark SQL and DataFrame:

1) Create a DataFrame from an existing RDD, from a Hive table or JSON/CSV files etc.
2) Register the DataFrame as a table.
3) Execute sql operations on the created table.

For more details, please refer:
https://spark.apache.org/docs/latest/sql-programming-guide.html

### GLOBAL HISTORICAL CLIMATE NETWORK (GHCN) - Daily Data Set

**GHCN-Daily** - An integrated database of daily climate summaries from land surface stations across the globe. GHCN-Daily is comprised of daily climate records from numerous sources that have been integrated and subjected to a common suite of quality assurance reviews.
It contains records from over 100,000 stations in 180 countries and territories. NCEI provides numerous daily variables, including maximum and minimum temperature, total daily precipitation, snowfall, and snow depth; however, about one half of the stations report precipitation only. Both the record length and period of record vary by station and cover intervals ranging from less than a year to more than 175 years.

In this learning lab, we will use the observations from global stations in 2017. The details are stored in two files viz. 
1. 2017.csv 
2. ghcnd-stations.txt

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/BASIC%20DATA%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/images/ClimateData1.jpeg?raw=true)


## PROCESS OVERVIEW 

![alt-tag](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/assets/images/Process16.jpeg?raw=true)

Please follow the steps given below to launch the workspace and execute the lab.

### **Step 1 : Select Learning Lab from DLP**

After launching the DLP dashboard page, navigate to learning labs tab. Select the learning lab "Climate Network Data Analysis with Spark SQL and Dataframes" and click on "Start" button as shown in screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/BASIC%20DATA%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/images/ClimateData1.jpeg?raw=true)

### **Step 2 : Workspace Page**

On click of Start button, user will be navigated to a workspace page where all the components - IDE, Tools and Microservices required to execute the program are available. Please refer the screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/BASIC%20DATA%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/images/ClimateData2.jpeg?raw=true)

### **Step 3 : Start Zeppelin and Map services in Workspace**

Points to Note:

* Start the Zeppelin service.Please refer screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/BASIC%20DATA%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/images/ClimateData3.jpeg?raw=true)

* Once started, the colour of the icon changes to green and launch button is enabled.

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/BASIC%20DATA%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/images/ClimateData4.jpeg?raw=true)

* If the task is in stopped status then click on it to start again. 

### **Step 4 : Launch Zeppelin**

Click on launch button and user will be navigated to Zeppelin home page.

Points to note:

(The pop-up may be blocked in browser configuration. Click on the red pop up blocker icon and select **Always allow pop-up from http://xxx.xxx.xxx.xxx ** and click on **Done** button.)  

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/DATA%20EXPLORATION%2C%20ANALYSIS%20AND%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/popUpBlockerAllowed1.jpeg?raw=true)

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/DATA%20EXPLORATION%2C%20ANALYSIS%20AND%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/popUpBlockerAllowed2.jpeg?raw=true)

On enabling the pop-up, Zeppelin would open in a new window taking the user to Zeppelin home page as shown in screenshot below.

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/BASIC%20DATA%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/images/ZeppelinVisualization12.jpeg?raw=true)


### **Step 5 : Select Zeppelin notebook**

* Select pre-created notebook - "Climate Network Data Analysis using Spark SQL and DataFrames" from home page as shown in screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/BASIC%20DATA%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/images/ClimateData5.jpeg?raw=true)

* The Zeppelin interactive,development and visualisation environment is now ready for use. The pre-created notebook has all the codes required for executing this learning lab. Each of the code shown below needs to be executed sequentially. 

### **Step 6 : Execute Scala Code**

We are using code written in Scala for data generation. 

The code is as follows:

```json

```
Following functions are being done by the Scala Code :

* Login to DLP and navigate to the workspace.
* Check the input files -2017.csv and ghcnd-stations.txt
* Start services – Zeppelin and Map View
* Launch Zeppelin and select the pre-created notebook
* Load Spark package
* Create dataframes using the input files and register tables (Observations and Stations)
* Query table Observations for a specific date (2017.01.01) and create a new table with the results - observations_20170101
* Perform Join operation (observations_20170101 and Stations)
* Query and Select core elements from the joined table. Perform further select operations.
* Save the result to HDFS(Preview output on DLP)
* Visualize data using Map service


Execute the code by clicking on the "Ready" icon highlighted as shown in screenshot below:. 

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/BASIC%20DATA%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/images/ClimateData6.jpeg?raw=true)

On successful execution of code, you can view the cosine signal data in Zeppelin as shown in screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/BASIC%20DATA%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/images/ClimateData7.jpeg?raw=true)

### **Step 7 : Geographical data visualization using Map service**

Launch the map service by clicking on the launch button. The map opens up in a separate tab as shown below:
![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/BASIC%20DATA%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/images/ClimateData8.jpeg?raw=true)

### **Step 8 : Stop Zeppelin and Map services in Workspace**


## LESSONS LEARNT

1. How to write and execute Scala code in Zeppelin.

2. How to use Apache Zeppelin to visualize data from two different tables.

3. How to use Spark SQL.

4. How to use Spark Dataframe.


# **Congratulations! You have successfully completed the Learning Lab!**
