# **INCOME ANALYSIS OF MASSACHUSETTS STATE**

**Programming Language Used** : Scala

**Executed on** : Hadoop Big Data Platform

**Visualization Tool Used** : Apache Zeppelin

## OVERVIEW

This learning lab can be used as a guide to do an income analysis of the entire population of MA State. The main objective here is to:
1. Read and analyze the 2011-2015 PUMS(Public Use  Microdata Sample) Massachusetts population data.
2. Build and evaluate a random forest model against the Massachusetts income data.
3. Conclude the major factors that contribute to the income difference

We will be using DevNet Data Learning Platform referred as "DLP" during the course. 

## PRE-REQUISITES

1.	Install Chrome Browser.

2.	Basic understanding of Hadoop Distributed File System (HDFS).

3.  Basic knowledge of SQL (Structured Query Language).

4.  Basic knowledge of Scala programming language.

5.  Obtain access to Data Learning Platform -
                                                          Refer: https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/HowToAccessDTLP.md


## LEARNING OBJECTIVES

1. To get familiarized with the DLP (Data Learning Platform).

2. To get familiarized with writing and executing Scala code in Zeppelin. 

3. To get familiarized with the usage of Apache Zeppelin to visualize data.

4. Get familiarized with the concepts and usage of SparkSQL, Dataframes and SparkMLlib.

5. Get familiarized with the Machine learning - Random Forest Model


## TERMINOLOGIES USED

### APACHE ZEPPELIN - AN INTRODUCTION

**Zeppelin is a web-based notebook that enables interactive data analytics.** You can make beautiful data-driven, interactive and collaborative documents with SQL, Scala and more. 

Zeppelin enables data engineers, data analysts and data scientists to be more productive by developing, organising, executing, and sharing data code and visualising results without referring to the command line or knowing the cluster details. It brings data exploration, visualization, sharing and collaboration features to Spark. It supports Python and also a growing list of programming languages such as Scala, Hive, SparkSQL, shell and markdown. The various languages are supported via Zeppelin language interpreters. Zeppelin’s notebooks provides interactive snippet-at-time experience to data scientist.

###### Key Features:

1. Web based notebook style editor.

2. Built-in Apache Spark support.

###### Use Cases for Zeppelin :

1. Data Exploration and discovery

2. Data Visualization - Tables,graphs and charts

3. Interactive snippet-at-a-time experience

4. Collaboration and publishing

For more details, please refer : https://zeppelin.apache.org/

### SCALA - AN INTRODUCTION

**Scala is an acronym for “Scalable Language”.** This means that Scala grows with you. Scala could be written by typing one-line expressions and observing the results and could also be used for large mission critical systems. Scala could also be considered as a scripting language and is a pure-bred object-oriented language. The language supports advanced component architectures through classes and traits. Even though its syntax is fairly conventional, Scala is also a full-blown functional language. Scala runs on the JVM. Java and Scala classes can be freely mixed, no matter whether they reside in different projects or in the same. Scala makes deliver things faster with less code.

For more details, please refer: http://www.scala-lang.org/what-is-scala.html https://en.wikipedia.org/wiki/Scala_(programming_language)

### HADOOP - AN INTRODUCTION

**Apache Hadoop is an open-source software framework for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware.** 

To understand Hadoop, there are two fundamental things about it - How Hadoop stores files and how it processes data. The framework that is used in Hadoop to process data is called MapReduce.

All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common and should be automatically handled by the framework. The core of Apache Hadoop consists of a storage part, known as Hadoop Distributed File System (HDFS), and a processing part called MapReduce. Hadoop splits files into large blocks and distributes them across nodes in a cluster.

Example : Imagine a file that is larger than the capacity of a computer then it would not be possible to store that file. Hadoop allows to store files bigger than what can be stored on one particular node or server. So it provides an ability to store very, very large files and also lets to store many, many files.

For more details, please refer: (https://en.wikipedia.org/wiki/Apache_Hadoop)

### SPARK SQL AND DATAFRAMES - AN INTRODUCTION 

**Spark SQL is a Spark module for structured data processing.** 

Spark SQL have these important features:

* Integrated - Spark SQL lets you query structured data inside Spark programs, using either SQL or a familiar DataFrame API. Usable in Java, Scala, Python and R.
* Uniform Data Access - DataFrames and SQL provide a common way to access a variety of data sources, including Hive, Avro, Parquet, ORC, JSON, and JDBC. You can even join data across these sources.
* Hive Integration - Spark SQL supports the HiveQL syntax as well as Hive SerDes and UDFs, allowing you to access existing Hive warehouses.
* Standard Connectivity - A server mode provides industry standard JDBC and ODBC connectivity for business intelligence tools.

**A DataFrame is a Dataset organized into named columns.**

It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. The DataFrame API is available in Scala, Java, Python, and R. In Scala and Java, a DataFrame is represented by a Dataset of Rows. In the Scala API, DataFrame is simply a type alias of Dataset[Row]. While, in Java API, users need to use Dataset to represent a DataFrame.

Basic steps to use Spark SQL and DataFrame:

1) Create a DataFrame from an existing RDD, from a Hive table or JSON/CSV files etc.
2) Register the DataFrame as a table.
3) Execute sql operations on the created table.

For more details, please refer:
https://spark.apache.org/docs/latest/sql-programming-guide.html

### SPARK MLlib - AN INTRODUCTION 

**Apache Spark's scalable machine learning library.**
MLlib is a low-level machine learning library that can be called from Scala, Python and Java programming languages. It has implementations for various common machine learning algorithms.

**Features**
* Scalable
* High Performance
* User-friendly API’s
* Many algorithms and utilities
* Integrates with Spark and other components seamlessly 

**Use Cases**
• Supply chain optimization and maintenance
• Advertising optimization- To find out the probability of users clicking on available ads.
• Marketing – To recommend products to customers to maximize revenue or engagement.
• Fraud Detection- To track the anomalous behavior of users.

### DATA SET - AN INTRODUCTION 
The Income Analysis of MA state from the US Government Open Data set provides the basis to evaluate the income of the entire population of Massachusetts. 
The data is obtained from  American Community Survey (ACS) that helps local officials, community leaders and businesses understand the changes taking place in their communities. It is the major source for detailed information about the American people and workforce. 

## PROCESS OVERVIEW 

![alt-tag](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/assets/images/Process17.jpeg?raw=true)

Please follow the steps given below to launch the workspace and execute the lab.

### **Step 1 : Select Learning Lab from DLP**

After launching the DLP dashboard page, navigate to learning labs tab. Select the learning lab "Income Analysis of Massachusetts State" and click on "Start" button as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis1.jpeg)

### **Step 2 : Workspace Page**

On click of Start button, user will be navigated to a workspace page where all the components - IDE, Tools and Microservices required to execute the program are available. Please refer the screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis2.jpeg)

### **Step 3 : Start Zeppelin and Map services in Workspace**

Points to Note:

* Start the Zeppelin service.Please refer screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis3.jpeg)

* Once started, the colour of the icon changes to green and launch button is enabled.

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis4.jpeg)

* If the task is in stopped status then click on it to start again. 

### **Step 4 : Launch Zeppelin**

Click on launch button and user will be navigated to Zeppelin home page.

Points to note:

(The pop-up may be blocked in browser configuration. Click on the red pop up blocker icon and select **Always allow pop-up from http://xxx.xxx.xxx.xxx ** and click on **Done** button.)  

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/DATA%20EXPLORATION%2C%20ANALYSIS%20AND%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/popUpBlockerAllowed1.jpeg?raw=true)

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/DATA%20EXPLORATION%2C%20ANALYSIS%20AND%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/popUpBlockerAllowed2.jpeg?raw=true)

On enabling the pop-up, Zeppelin would open in a new window taking the user to Zeppelin home page as shown in screenshot below:
![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis5.jpeg)


### **Step 5 : Select Zeppelin notebook**

* Select pre-created notebook - "open data - income" from home page as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis6.jpeg)

* The Zeppelin interactive,development and visualisation environment is now ready for use. The pre-created notebook has all the codes required for executing this learning lab. Each of the code shown below needs to be executed sequentially. 

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis7.jpeg)

### **Step 6 : Execute Scala Code**

We are using code written in Scala for data generation. 

Following functions are being done by the Scala Code :

**Load Spark Dependencies**
The code is as follows: 
```json
      %dep
      z.reset()
      z.addRepo("Spark Packages Repo").url("http://dl.bintray.com/spark-packages/maven")
      z.load("com.databricks:spark-csv_2.10:1.2.0")
```
Execute the code by clicking on the "Ready" icon highlighted as shown in screenshot below:. 

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis8.jpeg)

 On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis9.jpeg)

**Read the PUMS csv file from HDFS and pre-process it**

* Read the raw data and filter out the unwanted rows and columns.
The code is as follows: 
```json
%spark

// read in the PUMS 2011-2015 MA Income data
val path = sys.env("INPUT0_HDFS")
val income_pma = sqlContext.read
    .format("com.databricks.spark.csv")
    .option("header", "true")
    .option("inferSchema", "true")
    .load(path)
income_pma.persist()

scala.util.Try(sqlContext.dropTempTable("income_pma"))
income_pma.registerTempTable("income_pma")
println("The raw MA income data contains <" + income_pma.columns.length + "> columns and <" + income_pma.count + "> rows")

// filter out unnecessary rows and columns, drop N/A row
val raw = sqlContext.sql(query)
    .filter("TOT_INCOME>0").filter("LEVEL_OF_SCHOOLING>15")
    .drop("WEIGHT").drop("RT").drop("SERIALNO").drop("SPORDER").drop("ESR").drop("STATE_CODE")
    .na.drop
raw.persist()

println("The base data contains <" + raw.columns.length + "> clumns and <" + raw.count + "> rows - filter out unnecessary columns and NA records")
``` 
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis10.jpeg)

* Cut off 1% extremely low and 1% extremely high income cases(Remove data abnormalities or outliers)
The code is as follows: 
```json
import org.apache.spark.sql._

// calculate the percentiles
def calcPercentiles(df: DataFrame, col: String, percentiles: Array[Double]): Array[Double] = {
    val cdf = df.select(col).sort(col).rdd.map {
            case Row(value) => value match {
                case d : Double => d
                case i : Int => i.toDouble
            }
        }
        .zipWithIndex()
        .map(_.swap)
    
    val total = cdf.count
    percentiles.map{ p =>
        val idx = math.floor(p*total).toLong
        cdf.lookup(idx).head
    }
}

// cut of 1% extreme income cases, i.e., keep only [0.01, 0.99] income
val INCOME_PERCENTILES = Array(0.01, 0.99)
val cutoffs = calcPercentiles(raw, "TOT_INCOME", INCOME_PERCENTILES)
val data = raw.where($"TOT_INCOME">cutoffs(0) && $"TOT_INCOME"<=cutoffs(1))
data.persist()

println("Build data with <" + data.count + "> records - cutoff total income beteen [1%, 99%]")
``` 
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis11.jpeg)

**Understand and analyze the data**
* Define constraints and functions
The code is as follows: 
```json
import org.apache.spark.sql._

def showHistN(df: DataFrame, col_name: String, nbuckets: Int): Unit = {
    val rdd = df.select(col_name).rdd.map {
        case Row(value) => value match {
            case d : Double => d
            case i : Int => i.toDouble
        }
    }
    
    val (hv, hc) = rdd.histogram(nbuckets)
    println("%table\nvalue\tcount")
    val hist = hv.zip(hc).map{
        case (x, y) => println(x + "\t" + y)
    }
}

def showHistBuckets(df: DataFrame, col_name: String, buckets: Array[Double]): Unit = {
    val rdd = df.select(col_name).rdd.map {
        case Row(value) => value match {
            case d : Double => d
            case i : Int => i.toDouble
        }
    }
    
    val hc = rdd.histogram(buckets)
    println("%table\nvalue\tcount")
    val hist = buckets.zip(hc).map{
        case (x, y) => println(x + "\t" + y)
    }
}

val p_income = (0 to 15).toArray.map(_*10000.0) :+ 10000000.0
val p_age = (3 to 20).toArray.map(_*5.0)
val p_school = (16 to 25).toArray.map(_*1.0)
val p_weekly = (1 to 20).toArray.map(_*5.0)
val p_arrival = (0 to 30).toArray.map(_*10.0)
val p_depart = (0 to 30).toArray.map(_*5.0)

val options = Seq(("QUARTER_OF_BIRTH","Birth quarter"), ("SEX","Sex"), ("OWN_CHILD", "Own child"), ("MARRIED", "Merried"), ("SERVED_MILITARY", "Served military"), ("RACE", "Race"), ("DOUBLE_DEGREE", "Double degree"), ("PRIVATE_SECTOR", "Private sector"), ("NONPROFIT", "Noprofit"), ("STATEGOV", "State or government"), ("SELFDEPLOYED", "Self employed"), ("MANAGER", "Manager"), ("BUSINESS_GENERAL", "Business"), ("BUSINESS_FINANCIAL", "Financial"), ("INFORMATION_TECHNOLOGY", "Information technology"), ("ENGINEERING", "Engineering"), ("SCIENCE", "Science"), ("COMMUNITY", "Community"), ("LEGAL", "Legal"), ("EDUCATION", "Education"), ("MEDICINE", "Medicine"), ("HEALTHCARE", "Healthcare"), ("SECURITY", "Security"), ("FOOD", "Food"), ("CLEANING", "Cleaning"), ("SERVICES_GENERAL", "Services"), ("SALES", "Sales"), ("OFFICE_GENERAL", "Official"), ("AGRICULTURE", "Agriculture"), ("CONSTRUCTION", "Construction"), ("TECHNICIAN", "Technician"), ("OPERATORS", "Operators"), ("DRIVING", "Driving"), ("MILITARY", "Military"))
``` 
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis12.jpeg)
![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis13.jpeg)

* Understand continuous and categorical features

Continous Features:

The code is as follows: 
```json
data.describe("TOT_INCOME", "AGE", "LEVEL_OF_SCHOOLING", "WEEKLY_WORKING_HOURS", "WEEKS_WORKED_LAST_12MO", "ARRIVAL_WORK", "DEPARTURE_WORK").show()
``` 
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis14.jpeg)

Categorical Features:

The code is as follows: 
```json
%spark
import org.apache.spark.sql._

val option = z.select("Feature", options).asInstanceOf[String]
val result = data.groupBy(option).agg(count(option))

// show the value counts
println("%table\nvalue\tcount")
result.rdd.collect.foreach { case Row(x, y) =>
    println(x.asInstanceOf[Int] + "\t" + y.asInstanceOf[Long])
}
``` 
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis15.jpeg)

**Build and train a Random Forest (RF) model**

* Determine high income threshold

The code is as follows: 
```json
import org.apache.spark.ml.feature.VectorAssembler

val feature_names = data.columns.filter(_!="TOT_INCOME")
val high_income_threshold = z.input("High income threshold", "80000").asInstanceOf[String].toInt

// create "features" column, as DenseVector
val assembler = new VectorAssembler()
    .setInputCols(feature_names)
    .setOutputCol("features")

// create "label" column
val dataset = assembler
    .transform(data)
    .withColumn(
        "label",
        when(raw("TOT_INCOME")>high_income_threshold, 1.0).otherwise(0.0)
    )
dataset.persist()

println("Build dataset with <" + dataset.count + "> records, NA records have been skipped")
println("Assume HIGH_INCOME_THRESHOLD=" + high_income_threshold)
println("    people with low income: <" + dataset.where("label!=1.0").count + ">")
println("    people with high income: <" + dataset.where("label=1.0").count + ">")
``` 
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis16.jpeg)

* Cross-validation to train model on trainset

The code is as follows: 
```json
%spark

import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.classification.RandomForestClassifier
import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}
import org.apache.spark.ml.classification.RandomForestClassificationModel
import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer}
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator

val nFolds = 5
//val numTrees = Array(10, 50, 100, 200)
//val maxBins = Array(20, 60)
val numTrees = Array(10, 100)
val maxBins = Array(20)
val sMetric = "f1"

// define label indexer
val labelIndexer = new StringIndexer()
    .setInputCol("label")
    .setOutputCol("indexedLabel")
    .fit(dataset)

// define feature indexer
val featureIndexer = new VectorIndexer()
    .setInputCol("features")
    .setOutputCol("indexedFeatures")
    .setMaxCategories(8)
    .fit(dataset)

// split data into training data and testing data
val Array(trainData, testData) = dataset.randomSplit(Array(0.7, 0.3))

// define rf classifier
val rf = new RandomForestClassifier()
    .setLabelCol("indexedLabel")
    .setFeaturesCol("indexedFeatures")

// define label converter
val labelConverter = new IndexToString()
    .setInputCol("prediction")
    .setOutputCol("predictedLabel")
    .setLabels(labelIndexer.labels)

// build pipeline
val pipeline = new Pipeline()
    .setStages(Array(labelIndexer, featureIndexer, rf, labelConverter))

// define the param grid
val paramGrid = new ParamGridBuilder()
    .addGrid(rf.maxBins, maxBins)
    .addGrid(rf.numTrees, numTrees)
    .build()

val evaluator = new MulticlassClassificationEvaluator()
    .setLabelCol("indexedLabel")
    .setPredictionCol("prediction")
    .setMetricName(sMetric)

// build up the cross validation pipeline
val cv = new CrossValidator()
    .setEstimator(pipeline)
    .setEvaluator(evaluator)
    .setEstimatorParamMaps(paramGrid)
    .setNumFolds(nFolds)

// train the rf model
val model = cv.fit(trainData)
``` 
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis17.jpeg)
![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis18.jpeg)

**Predict and evaluate the Random Forest model**

* Predicate test data and show some result

The code is as follows: 
```json
val predictions = model.transform(testData)
predictions.select("predictedLabel", "label", "features").show()
   ``` 
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis19.jpeg) 

* Evaluate the prediction result by accuracy, recall, and f1   

The code is as follows: 
```json
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator

// calculate accuracy
val evaluator = new MulticlassClassificationEvaluator()
  .setLabelCol("indexedLabel")
  .setPredictionCol("prediction")
  .setMetricName("precision")
val accuracy = evaluator.evaluate(predictions)
println("Test Error = " + (1.0 - accuracy))

// calculate recall
evaluator.setMetricName("recall")
val recall = evaluator.evaluate(predictions)
println("Test Recall = " + recall)

// calculate f1
evaluator.setMetricName("f1")
val f1 = evaluator.evaluate(predictions)
println("f1 value = " + f1)
   ``` 
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis20.jpeg) 

* Calculate confusion matrix

The code is as follows: 
```json
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
import org.apache.spark.mllib.evaluation.MulticlassMetrics

val predictionAndLabels = predictions.select("indexedLabel", "predictedLabel")
    .map { case Row(l, p) =>
        (l.asInstanceOf[Double], p.asInstanceOf[String].toDouble)
    }

val metrics = new MulticlassMetrics(predictionAndLabels)
println("Confusion matrix:")
println(metrics.confusionMatrix))
   ``` 
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis21.jpeg) 

**Conclude on the factors that contribute most to the income difference**

The code is as follows: 
```json
import org.apache.spark.ml.PipelineModel
import org.apache.spark.ml.classification.RandomForestClassificationModel

val rfm = model.bestModel.asInstanceOf[PipelineModel].stages(2).asInstanceOf[RandomForestClassificationModel]
val fnames = data.columns.filter(_!="TOT_INCOME")
val importances = rfm.featureImportances.toArray
val result = fnames.zip(importances).sortBy(- _._2)
val limit = z.input("Limit", "8").asInstanceOf[String].toInt

println("%table\nfeature\tGini importance")
for (i <- 0 until limit) {
    val (f, imp) = result(i)
    println(f + "\t" + imp)
}
``` 
On successful execution of code, you can view the details as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis22.jpeg) 


### **Step 8 : Stop Zeppelin and Map services in Workspace**

After finishing the code execution on zeppelin, we need to stop the services from DLP.

Please refer screenshot below:
![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/CLIMATE%20NETWORK%20DATA%20ANALYSIS%20FOR%20STORM%20TRACKING%20WITH%20SPARK%20SQL%20AND%20DATAFRAMES/assets/images/IncomeAnalysis23.jpeg)


## LESSONS LEARNT

1. How to write and execute Scala code in Zeppelin.

2. How to use Apache Zeppelin to visualize data from two different tables.

3. How to use Spark SQL, Dataframe and SparkMLlib.

# **Congratulations! You have successfully completed the Learning Lab!**
