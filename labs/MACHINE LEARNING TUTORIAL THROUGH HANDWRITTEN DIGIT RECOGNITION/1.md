# MACHINE LEARNING TUTORIAL THROUGH HANDWRITTEN DIGIT RECOGNITION 

<b>Programming Language Used</b> : Python

<b>Executed on</b> : Hadoop Big Data Platform

<b>Visualization Tool Used</b> : Apache Zeppelin

## OVERVIEW

This learning lab can be used as a guide to get a high level understanding to use machine learning in order to letÿdata drive decisioning. We will use a dataset of tens of thousands of images of handwritten numbers, allowing the models to learn how to decide what digit each image represents.
This is aÿsupervised classificationÿmodel, since we are trying to group the images into any of the digits 0 through 9 and we have labels for those digits.
This task, like many machine learning and artificial intelligence tasks, has powerful practical implications. Here we are using theÿMNIST dataset, which wasÿcollected by the United States Census Bureau. Many organizations, such as theÿUS Post Office, are using machine learning algorithms currently in order to provide more accurate and efficient reading of handwritten addresses and zip codes.
We will be using DevNet Data Learning Platform referred as "DLP" during the course. 

In this lesson we will learn how 


## PRE-REQUISITES

1. Install Chrome Browser.

2. Basic understanding of Apache Hadoop and Big Data.

3. Basic knowledge of Python.


## LEARNING OBJECTIVES

1. To get familiarized with the DLP (Data Learning Platform).

2. To get familiarized with writing and executing python code in Zeppelin. 

3. To get a high level understanding to use machine learning.


## TERMINOLOGIES USED

### APACHE ZEPPELIN - AN INTRODUCTION

Zeppelin is a web-based notebook that enables interactive data analytics. You can make beautiful data-driven, interactive and collaborative documents with SQL, Scala and more. </br>

Zeppelin enables data engineers, data analysts and data scientists to be more productive by developing, organising, executing, and sharing data code and visualising results without referring to the command line or knowing the cluster details. It brings data exploration, visualization, sharing and collaboration features to Spark. It supports Python and also a growing list of programming languages such as Scala, Hive, SparkSQL, shell and markdown. The various languages are supported via Zeppelin language interpreters. Zeppelinâ€™s notebooks provides interactive snippet-at-time experience to data scientist.

###### Key Features:

1. Web based notebook style editor.

2. Built-in Apache Spark support.

###### Use Cases for Zeppelin :

1. Data Exploration and discovery

2. Data Visualization - Tables,graphs and charts

3. Interactive snippet-at-a-time experience

4. Collaboration and publishing

For more details, please refer : https://zeppelin.apache.org/



### DLP - AN INTRODUCTION ###

The DevNet Data Learning Platform (DLP) is an integrated data platform from CISCO that includes an easy-to-use UI, Docker-    based infrastructure, best-in-class open-source big-data components, and Ciscoâ€™s APIs and tools for data developers and data  scientists who want to develop, validate and provision their solutions before deploying or to explore, analyze, and    visualize their data. The DLP environment comes with an inbuilt cloud based IDE (Integrated Development Environment) built    on Hadoop.

For more details, please refer:
https://developer.cisco.com/site/dlp/docs/overview/

## PROCESS OVERVIEW 

![alt-tag](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/assets/images/Process3.jpeg?raw=true)

Please follow the steps given below to launch the workspace and execute the program.


### <b>Step 1 : Login to DLP</b>

Access the link - https://developer.cisco.com/site/dlp/ and click on the button "Access". Please refer the screen shown below:</br>

![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/assets/images/UserAccess1.png?raw=true)

On click of Access, you will be navigated to the following page:

<img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/assets/images/UserAccess2.jpeg" data-canonical-src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/assets/images/UserAccess2.jpeg" width="400" height="300" />

<b>Existing User?</b>

Click on the button shown below and provide the credentials in login page. Please refer the screen below:

<img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/assets/images/UserAccess7.jpeg" data-canonical-src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/assets/images/UserAccess7.jpeg" width="400" height="300" />

On click of button:

![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/WORD COUNT USING SCALA WITH APACHE SPARK/assets/images/UserAccess3.jpeg?raw=true)

<b>New User?</b>

Click on â€œRegister Accountâ€ button and create a new login profile. Please refer the screen below:

<img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/assets/images/UserAccess8.jpeg" data-canonical-src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/assets/images/UserAccess8.jpeg" width="400" height="300" />

On click of "Register Account":

![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/WORD COUNT USING SCALA WITH APACHE SPARK/assets/images/UserAccess5.jpeg?raw=true)


<img src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/assets/images/UserAccess6.jpeg" data-canonical-src="https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/assets/images/UserAccess6.jpeg" width="300" height="400" />

On successful registration, navigate back to the DLP login page and login with the credentials created:

![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/WORD COUNT USING SCALA WITH APACHE SPARK/assets/images/UserAccess3.jpeg?raw=true)

### <b>Step 2 : DLP Dashboard Page</b>

On login, you will be directed to the DevNet DLP (Data Learning Platform)dashboard page as shown in the screenshot below:

![alt-tag](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/WORD COUNT USING SCALA WITH APACHE SPARK/assets/images/UserAccess4.jpeg?raw=true)

### <b>Step 3 : Select Learning Lab and Start</b>

From Learning Labs pane, select the learning lab "Machine Learning tutorial through Handwritten Digit Recognition" and click on "Start" button as shown in screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning1.jpeg?raw=true)

### <b>Step 4 : Workspace Page</b>

On click of Start button, user will be navigated to a workspace page where all the components - IDE, Tools and Microservices required to execute the program are available. Please refer the screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning2.jpeg?raw=true)

### <b>Step 5 : Start Service in Workspace</b>

Points to Note:

* Start the Zeppelin service.Please refer screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning3.jpeg?raw=true)

* Once started, the colour of the icon changes to green and launch button is enabled.

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning4.jpeg?raw=true)

* If the task is in stopped status then click on it to start again. 

### <b>Step 6 : Launch Zeppelin</b>

Click on launch button and user will be navigated to Zeppelin home page.

Points to note:

(The pop-up may be blocked in browser configuration. Click on the red pop up blocker icon and select <b>Always allow pop-up from http://xxx.xxx.xxx.xxx </b> and click on <b>Done</b> button.) </br> 

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/DATA%20EXPLORATION%2C%20ANALYSIS%20AND%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/popUpBlockerAllowed.PNG?raw=true)

On enabling the pop-up, Zeppelin would open in a new window taking the user to Zeppelin home page with pre-configured notebook - "Cisco DevNet DLP Machine Learning" as shown in screenshot below.

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning5.jpeg?raw=true)


### <b>Step 7 : Select Zeppelin notebook</b>

* Select pre-created notebook - "Cisco DevNet DLP Machine Learning" from home page and below screen would be shown to the user:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning6.jpeg?raw=true)


* The Zeppelin interactive,development and visualisation environment is now ready for use. The notebook - "zeppelin-data-visualization-learning-labs" has all the codes required for executing this learning lab. Each of the code shown below needs to be executed sequentially. 

### <b>Step 8 : Prepare for Modeling</b>

<b>Access and Format data</b>

```json
%python

### Below is a function to read in the image and label data.

### Inspired by the following gist: https://gist.github.com/akesling/5358964
import os
import struct
import numpy as np
import re
import random
from __future__ import division

def read(dataset = "training", path = "."):
    """
    Python function for importing the MNIST data set.  It returns an iterator
    of 2-tuples with the first element being the label and the second element
    being a numpy.uint8 2D array of pixel data for the given image.
    """
    if dataset == "training":
        img_file = os.path.join(path, 'train-images-idx3-ubyte')
        lbl_file = os.path.join(path, 'train-labels-idx1-ubyte')
    elif dataset == "testing":
        img_file = os.path.join(path, 't10k-images-idx3-ubyte')
        lbl_file = os.path.join(path, 't10k-labels-idx1-ubyte')
    else:
        raise ValueError("dataset must be 'testing' or 'training'")
    # Load everything in some numpy arrays
    with open(lbl_file, 'rb') as lbl_fc:
        (magic, num) = struct.unpack(">II", lbl_fc.read(8))
        lbl = np.fromfile(lbl_fc, dtype=np.int8)
    with open(img_file, 'rb') as img_fc:
        (magic, num, rows, cols) = struct.unpack(">IIII", img_fc.read(16))
        img = np.fromfile(img_fc, dtype=np.uint8).reshape(len(lbl), rows, cols)
    # If we wanted an iterator which returns each image in turn
    # get_img = lambda idx: (lbl[idx], img[idx])
    #    for i in xrange(len(lbl)):
    #        yield get_img(i)
    return (lbl, img)


### Here is a function to display images
def show(image, lbl=None, prd=None, idx=None):
    """
    Render a given numpy.uint8 2D array of pixel data.
    """
    from matplotlib import pyplot
    import matplotlib as mpl
    plt_title = ""
    fig = pyplot.figure()
    ax = fig.add_subplot(1,1,1)
    imgplot = ax.imshow(image, cmap=mpl.cm.Greys)
    imgplot.set_interpolation('nearest')
    ax.xaxis.set_ticks_position('top')
    ax.yaxis.set_ticks_position('left')
    if lbl is not None:
        plt_title += "Label: %d" % lbl
    if prd is not None:
        if len(plt_title) > 0:
            plt_title += "; Prediction: %d" % prd
        else:
            plt_title += "Prediction: %d" % prd
    if idx is not None:
        if len(plt_title) > 0:
            plt_title += "; Image Index: %d" % idx
        else:
            plt_title += "Image Index: %d" % idx
    if len(plt_title) > 0:
        pyplot.title(plt_title, y=1.05)
    z.show(pyplot)
    pyplot.close()


def scale(array, method='zero-one'):
    """Scale the data so that it can work more easily with algorithms like SVM,
    which require scaling. For now, this only allows for scaling between the
    range of [0,1], inclusive.
    This assumes a numpy array is passed in.
    """
    if method != 'zero-one':
        raise NotImplementedError("Only 'zero-one' method currently implemented.")
    array = array - array.min()
    array = array / array.max()
    return array


def zepp_print_array(array, prefix="%table ", orig_sep=r"([-0-9\.]) +([-0-9])",
                     new_sep=r"\1\t\2", remove=r"[\[\]]", headers=None):
    """Prepare a string that is ready to be printed with Zeppelin's nice table
    formatting.
    """
    if len(array.shape) != 2:
        raise NotImplementedError("'zepp_print_array' can currently only print 2D arrays.")
    if headers is None:
        headers = list(range(array.shape[1]))
        headers = ['Digit ' + str(header) for header in headers]
    if isinstance(headers, list) or isinstance(headers, tuple):
        headers = "\t".join(headers)
    if not isinstance(headers, str):
        headers = str(headers)
    str_array = np.array_str(array) # Convert ndarray to a string
    #    sep_re = re.compile(orig_sep)
    str_array = re.sub(orig_sep, new_sep, str_array)
    str_array = re.sub(orig_sep, new_sep, str_array)
    str_array = re.sub(r' ', '', str_array)
    str_array = re.sub(remove, "", str_array)
    str_array = prefix + headers + '\n' + str_array
    print(str_array)


def gen_random(size):
    while True:
        yield random.randrange(size)


def gen_next(*args):
    if len(args) == 1:
        for next_val in range(args[0]):
            yield next_val
    elif len(args) == 2:
        for next_val in range(args[0], args[1]):
            yield next_val
    elif len(args) == 3:
        for next_val in range(args[0], args[1], args[2]):
            yield next_val


# First we will read in the data and get it in the format / shape that we'll need.
(lbls, orig_imgs) = read(dataset="testing", path="/tmp")


# Now flatten the imgs one dimension.  We'll use an algorithm that does not require local neighborhood knowledge, so
# each pixel is simply considered a feature.
imgs = orig_imgs.reshape(len(orig_imgs), -1)

# We also need to scale our image, since SVM is impacted by scale. The following call
# will scale all image values to [0,1].
imgs = scale(imgs)

# Now create some generators that we'll use to look through images.
random_num = gen_random(len(lbls))
next_num = gen_next(len(lbls))
```
Execute the code by clicking on the "Ready" icon highlighted as shown in screenshot below:. 

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning7.jpeg?raw=true)

On successful execution of code, the icon changes to "Finished" as shown in screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning.jpeg?raw=true)


### <b>Step 9 : Look at Example Image</b>

Below we look at one of the images, just to get a sense of what we're looking at. The paragraph below is designed to let you keep re-running it to look at different images.
Theÿindexÿof the image is referring to which image number, of the 10,000 available, you are looking at.
In this case, we will look at each image in order. (The first image, then the second, then third, etc.)

The code is as follows:

```json
%python
next_idx = next(next_num)
show(orig_imgs[next_idx], lbls[next_idx], idx=next_idx)
```

Execute the code by clicking on the "Ready" icon highlighted as shown in screenshot below:. 

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning8.jpeg?raw=true)

On successful execution of code, the icon changes to "Finished" as shown in screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning.jpeg?raw=true)



### <b>Step 9 : Look at Random Example Image</b>

Below we again look at one of the images, but this time it is randomly chosen, to give a better sense of variety.
Again, the code below can be re-run to examine different images (each randomly chosen).

```json
%python
random_idx = next(random_num)
show(orig_imgs[random_idx], lbls[random_idx], idx=random_idx)
```
Execute the code by clicking on the "Ready" icon highlighted as shown in screenshot below:. 

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning9.jpeg?raw=true)

On successful execution of code, the icon changes to "Finished" as shown in screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning8.jpeg?raw=true)



### <b>Step 10 : Split Training and Validation Data</b>


Below we define the training and validation datasets. We define these by capturing an array of the indices for each set.
Machine learning isÿdata-driven modeling. As such, the data is the most precious part. We need to use the dataÿbothÿto train / create our model, as well as to validate the model.
Toÿprevent overfitting, as discussed in the lecture portion of this lab, we need to validate on an entirely different data set than that we use to train.

```json
%python
# Split the data into training and test data sets.
train_set_size = int(len(lbls) * 0.5)
train_idx = list(range(train_set_size))
test_idx = list(range(train_set_size, len(lbls)))
```

Execute the code by clicking on the "Ready" icon highlighted as shown in screenshot below:. 

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning10.jpeg?raw=true)

On successful execution of code, the icon changes to "Finished" as shown in screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning.jpeg?raw=true)



### <b>Step 11 : Modeling</b>

Below the modeling begins. We are using support vector machine(SVM) models. We are performing aÿsupervised classification model. The model is supervisedÿbecause we have a specified goal, and training data which provide answers to that goal. Specifically, we have handwritten digits which are to be recognized as particular digits, and we have the answers to those digits, as shown in the image above.
The model isÿclassifyingÿbecause there are particular, discrete bins that we are trying to fit the predictions into. Here we are trying to tell to which of the numbers 0 - 9 the handwritten digits look most similar. This might seem like regression, since we are talking about numbers, but it is classification, because each number looks drastically different from its neighbor. The number '1' mightÿseemÿsimilar to '0' or '2' in our minds, but as it is written, they look nothing alike. In fact, '0' looks most similar to '8', and maybe '1' looks most similar to '7'. We'll see, in the validation, that in fact the most frequent mix-ups are with digits thatÿlookÿmost similar.

Find Model Parameters

We will first run several differentÿsupport vector machineÿ(SVM) models, changing the parameters to find which work best. Specifically, we look at aÿpolynomialÿkernel versus aÿradialÿkernel, as well as several different values for theÿpenalty parameter, 'C'.When modeling, we only use the training data. The remaining data we will use for final validation for assessing the model quality at the end.
Necessarily, this work also generates the model as well.

The modeling performs a 3-foldÿcross-validation, further breaking the training dataset into 2 groups, one of which is used for model cross-validation. This is different from the final validation mentioned immediately above.
The code below takes aÿlongÿtime to run, potentially up to 10 minutes.

```json
%python
# Here we will optimize the "hyper-parameters" of our model, *if needed*.  Optimizing these
# parameters takes a long time, so this should only be run when needed.
# We will use a Support Vector Machine (SVM) algorithm.

from sklearn.model_selection import GridSearchCV # This is the newer library, might be needed instead.
from sklearn import svm, metrics
#from sklearn.grid_search import GridSearchCV

parameters = {'kernel':('poly', 'rbf'), 'C':[1, 10, 100]}
estimator = svm.SVC()
classifier = GridSearchCV(estimator, parameters)
classifier.fit(imgs[[train_idx]], lbls[[train_idx]])

GridSearchCV(cv=None, error_score='raise',
estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
max_iter=-1, probability=False, random_state=None, shrinking=True,
tol=0.001, verbose=False),
fit_params={}, iid=True, n_jobs=1,
param_grid={'kernel': ('poly', 'rbf'), 'C': [1, 10, 100]},
pre_dispatch='2*n_jobs', refit=True, return_train_score=True,
scoring=None, verbose=0)
```

Execute the code by clicking on the "Ready" icon highlighted as shown in screenshot below:. 

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning11.jpeg?raw=true)

On successful execution of code, the icon changes to "Finished" as shown in screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning.jpeg?raw=true)



### <b>Step 11 : Assess Model Quality</b>
The model has been completed. In the next few sections, we'll assess the model both by rigorous mathematical calculations and assessments, as well as studying a sampling of what went wrong.
Examine ?Confusion Matrix?
We will first look at theÿconfusion matrix, a fancy phrase for a fairly simple concept.

Along the columns, we have the count of images with thatÿpredictedÿdigit, and across the rows, we have the count for theÿactualÿdigit. For instance, the upper-left corner represents the number of images that were labelled as 0's, and that were successfullyÿpredictedÿto be zeros. The upper-right corner represents the number of images that were labelled as 0's and that wereÿincorrectly predictedÿto be 9's.

Therefore, theÿdiagonalÿfrom upper-left to lower-right represents all of the digits that wereÿpredicted properly, or which the machine learning algorithm properly modelled. Anythingÿnotÿon the diagonal was improperly modeled.
This code below, again, can potentially take up to 10 minutes.
```json
%python
from sklearn import metrics
predicted = classifier.predict(imgs)
print(metrics.confusion_matrix(lbls[[test_idx]], predicted[[test_idx]]))

[[509 0 0 1 2 0 2 3 3 0]
[ 0 552 0 0 0 1 1 0 10 0]
[ 8 0 453 1 19 2 4 3 9 3]
[ 2 0 20 443 1 8 0 4 28 4]
[ 1 0 0 0 471 0 2 2 2 4]
[ 2 4 5 12 17 377 12 1 6 0]
[ 0 0 6 0 5 0 485 0 0 0]
[ 2 2 5 0 2 0 0 481 0 24]
[ 0 4 3 0 3 8 7 4 455 1]
[ 2 0 0 0 30 11 0 7 5 434]]
```

Execute the code by clicking on the "Ready" icon highlighted as shown in screenshot below:. 

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning12.jpeg?raw=true)

On successful execution of code, the icon changes to "Finished" as shown in screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning.jpeg?raw=true)


### <b>Step 9 : Plot cosine wave</b>
The Classification Report
There are several other methods to validate the quality of the model. The method given below is very powerful, but as we delve further into these mathematical methods, they become more complex. We won't delve into all of it, but we will touch upon theÿprecisionÿandÿrecall.

Wikipediaÿhas a nice discussion on the precision and recall, given below. In short, theÿprecisionÿis the percent of things that you predicted that were correctly predicted. E.g., if 500 images wereÿpredictedÿto be 0's, and 490 of themÿwere actually labelled as zeros, then the precision would be 98% (490 / 500).
The recall is the percent of things that truly were a value, that you correctly predicted to be that value. E.g., if 1000 images wereÿlabelledÿas 0's, and 950 of themÿwere predicted as zeros, then the precision would be 95%.

```json
READYÿÿÿ
%python
print(metrics.classification_report(lbls[[test_idx]], predicted[[test_idx]]))
precision recall f1-score support
0 0.97 0.98 0.97 520
1 0.98 0.98 0.98 564
2 0.92 0.90 0.91 502
3 0.97 0.87 0.92 510
4 0.86 0.98 0.91 482
5 0.93 0.86 0.89 436
6 0.95 0.98 0.96 496
7 0.95 0.93 0.94 516
8 0.88 0.94 0.91 485
9 0.92 0.89 0.91 489
avg / total 0.93 0.93 0.93 5000

```
Execute the code by clicking on the "Ready" icon highlighted as shown in screenshot below:. 

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning13.jpeg?raw=true)

On successful execution of code, the icon changes to "Finished" as shown in screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning.jpeg?raw=true)

### <b>Step 9 : Model Overfitting</b>

In the lesson slides, we had discussed model overfitting. In the work here, we addressed model overfitting in two ways:
1. We internally assessed the SVM model quality via a 3-fold cross-validation
2. When we looked at the performance of the model, weÿonlyÿcompared the predictive capabilities of the validation dataset, the dataset that was never used for any modeling in any way.
Let's break that second rule above, and instead evaluate the performance by looking at how well the model works in predicting theÿtraining dataset. This will lead us to overconfidence of the quality of our models, when evaluating, and when internally assessing, it could also allow for overfitting.

```json
%python

#print(metrics.confusion_matrix(lbls[[train_idx]], predicted[[train_idx]]))
print(metrics.classification_report(lbls[[train_idx]], predicted[[train_idx]]))
precision recall f1-score support
0 0.97 0.98 0.98 460
1 0.97 0.99 0.98 571
2 0.97 0.95 0.96 530
3 0.95 0.95 0.95 500
4 0.93 0.97 0.95 500
5 0.95 0.95 0.95 456
6 0.97 0.97 0.97 462
7 0.95 0.94 0.94 512
8 0.95 0.94 0.95 489
9 0.95 0.92 0.93 520
avg / total 0.96 0.96 0.96 5000
```
Execute the code by clicking on the "Ready" icon highlighted as shown in screenshot below:. 

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning14.jpeg?raw=true)

On successful execution of code, the icon changes to "Finished" as shown in screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning.jpeg?raw=true)

Overall the precision and recall values areÿmuchÿcloser to 100%. But this does not represent the truth. We will never want to know how well a model can predict data we have already fed it, but instead future data that is being seen for the first time by the model.
Delving into Misclassified Images
Finally, let's look at some of those images that were misclassified (within the test set).
It's often worthwhile to look at the images that were misclassified, to better understand what might have gone wrong. Were the misclassifications because some of the handwritten digits were justÿreallyÿhard to read? Were they misclassified because there is some systematic mistake happening, such as 2's and 3's consistently being mistaken? Sometimes it helps simply to drive down into some of the errors.

First we find the indices of the images that were misclassified.
```json
%python

mislbld_idx = [idx for expct, pred, idx in zip(lbls[[test_idx]], predicted[[test_idx]], test_idx) if expct != pred]
img_gen = gen_next(len(mislbld_idx))
```
Execute the code by clicking on the "Ready" icon highlighted as shown in screenshot below:. 

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning15.jpeg?raw=true)

On successful execution of code, the icon changes to "Finished" as shown in screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning.jpeg?raw=true)


Then we show a misclassified image, in a manner similar to what we'd done earlier, where we can keep re-running the code below in order to see the next misclassified image.


```json
%python

next_img = mislbld_idx[next(img_gen)]
show(orig_imgs[next_img], lbls[next_img], predicted[next_img], idx=next_img)
```
Execute the code by clicking on the "Ready" icon highlighted as shown in screenshot below:. 

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning16.jpeg?raw=true)

On successful execution of code, the icon changes to "Finished" as shown in screenshot below:

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/tree/master/labs/MACHINE%20LEARNING%20TUTORIAL%20THROUGH%20HANDWRITTEN%20DIGIT%20RECOGNITION/assets/images/Machinelearning.jpeg?raw=true)


## LESSONS LEARNT

In this lesson we walked through machine learning modeling, where we predicted handwritten digits, automatically assigning a value to the handwritten digit. This sort of task is not only useful, but already being used within the industry, such asÿby the post office to better deliver mailÿto its customers.

This work involved:
1. Some preparatory work that we did not delve deeply into, such as reading the data.
2. Visually looking at our digit images, so that we can better understand the task at hand.
3. Creating a training and testing dataset.
4. Generating the model.This was done either with a wider search for model parameters, or using preset parameters.
5. Predicting digits based upon theÿtest datasetÿimages.
6. Examining the quality of those predictions in multiple ways. Including a slight divergence to show an inaccurate means of    assessing quality by using the training dataset.
7. Finally delving into some of the misclassified images to understand what may have caused the problems.



# <center>Congratulations! You have successfully completed the Learning Lab!
