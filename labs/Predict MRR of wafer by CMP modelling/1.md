# **PREDICT MRR OF WAFER BY CMP MODELLING**

**Programming Language Used** : Python

**Executed on** : Hadoop Big Data Platform

**Visualization Tool Used** : Apache Zeppelin

## OVERVIEW

This learning lab can be used as a guide to predict MRR(Material Removal Rate) of wafers by modelling CMP(Chemical Mechanical Planarization). The main objective here is to get familiar with: 
1. What is CMP (Chemical Mechanical Planarization)?
2. How to measure and inspect CMP?
3. How to predict MRR (Material Removal Rate) by modelling CMP?

We will be using DevNet Data Learning Platform referred as "DLP" during the course. 

## PRE-REQUISITES

1. Install Chrome Browser.

2. Basic knowledge of Python.

3. Basic knowledge of Machine Learning.

4. Obtain access to Data Learning Platform -
                                                          Refer: https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/HowToAccessDTLP.md


## LEARNING OBJECTIVES

1. To get familiarized with the DLP (Data Learning Platform).

2. To get familiarized with writing and executing python code in Zeppelin. 

## TERMINOLOGIES USED

### APACHE ZEPPELIN - AN INTRODUCTION

Zeppelin is a web-based notebook that enables interactive data analytics. You can make beautiful data-driven, interactive and collaborative documents with SQL, Scala and more. 

Zeppelin enables data engineers, data analysts and data scientists to be more productive by developing, organising, executing, and sharing data code and visualising results without referring to the command line or knowing the cluster details. It brings data exploration, visualization, sharing and collaboration features to Spark. It supports Python and also a growing list of programming languages such as Scala, Hive, SparkSQL, shell and markdown. The various languages are supported via Zeppelin language interpreters. ZeppelinΓÇÖs notebooks provides interactive snippet-at-time experience to data scientist.

###### Key Features:

1. Web based notebook style editor.

2. Built-in Apache Spark support.

###### Use Cases for Zeppelin:

1. Data Exploration and discovery

2. Data Visualization - Tables, graphs and charts

3. Interactive snippet-at-a-time experience

4. Collaboration and publishing

For more details, please refer: https://zeppelin.apache.org/


## PROCESS OVERVIEW 

![alt-tag](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/assets/images/ProcessXX.jpeg?raw=true)

Please follow the steps given below to launch the workspace and execute the lab.

### **Step 1 : Select Learning Lab from DLP**

After launching the DLP dashboard page, navigate to learning labs tab. Select the learning lab "Predict MRR of Wafer by CMP Modelling" and click on "Start" button as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/TENSORFLOW101/assets/images/MRR1.jpeg?raw=true)

### **Step 2 : Workspace Page**

On click of Start button, user will be navigated to a workspace page where all the components - IDE, Tools and Microservices required to execute the program are available. Please refer the screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/TENSORFLOW101/assets/images/MRR2.jpeg?raw=true)

### **Step 3 : Start Service in Workspace**

Points to Note:

* Start the Zeppelin service.Please refer screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/TENSORFLOW101/assets/images/MRR3.jpeg?raw=true)

* Once started, the colour of the icon changes to green and launch button is enabled.

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/TENSORFLOW101/assets/images/MRR4.jpeg?raw=true)

* If the task is in stopped status then click on it to start again. 

### **Step 4 : Launch Zeppelin**

Click on launch button and user will be navigated to Zeppelin home page.

Points to note:

(The pop-up may be blocked in browser configuration. Click on the red pop up blocker icon and select **Always allow pop-up from http://xxx.xxx.xxx.xxx ** and click on **Done** button.)  

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/DATA%20EXPLORATION%2C%20ANALYSIS%20AND%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/popUpBlockerAllowed1.jpeg?raw=true)

![Figure](https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/DATA%20EXPLORATION%2C%20ANALYSIS%20AND%20VISUALIZATION%20USING%20APACHE%20ZEPPELIN/assets/popUpBlockerAllowed2.jpeg?raw=true)

On enabling the pop-up, Zeppelin would open in a new window taking the user to Zeppelin home page as shown in screenshot below.

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/TENSORFLOW101/assets/images/MRR5.jpeg?raw=true)
The following notebooks are provided in zeppelin:

1.	Analysis and modeling
2.	Wafer dbn(Deep Belief Network) & xqboost
3.	Wafer physical model


### **Step 5 : Select Zeppelin notebook – “Analysis and Modeling”**


The analysis is done in Pyspark, Spark ML and python. We use physical model, Linear regression Classifier, DBN and XGBoost to model CMP data.

* Select pre-created notebook - "Analysis and modeling" from home page as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/TENSORFLOW101/assets/images/MRR6.jpeg?raw=true)

* The Zeppelin interactive,development and visualisation environment is now ready for use. The pre-created notebook has all the codes required for executing this learning lab. 

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/TENSORFLOW101/assets/images/MRR7.jpeg?raw=true)



### **Step 6 : Execute the code in notebook – “Analysis and Modeling”**
We are using code written in Pyspark, Spark ML and python. 

Following functions are being done by the Code :

1.	Read and analyze CMP data
2.	Build and evaluate a linear regression model against CMP data
3.	Compare the results between Statistical approach and physics-based modeling approach.


Execute the code by clicking on the icon highlighted as shown in screenshot below:. 
  
![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/TENSORFLOW101/assets/images/MRR7.jpeg?raw=true)

On successful execution of the code, the icon changes to "Finished" as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/TENSORFLOW101/assets/images/MRR8.jpeg?raw=true)


The code is as follows:

1.	Load Dependencies

```
%dep
z.reset()
z.addRepo("Spark Packages Repo").url("http://dl.bintray.com/spark-packages/maven")
z.load("com.databricks:spark-csv_2.10:1.2.0") 
```


2.	Load Raw Data
```
%pyspark
import os

train_df = (sqlContext.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true")
                                                                  .load(os.environ['HDFS_ROOT'] + "/cmp-train-all.csv"))
                                                                  
train_rm_df = (sqlContext.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true")
                                                                 .load(os.environ['HDFS_ROOT'] + "/cmp-training-removalrate.csv"))
test_df=(sqlContext.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true")
                                                                 .load(os.environ['HDFS_ROOT'] + "/CMP_test_data.csv"))
test_rm_df=(sqlContext.read.format("com.databricks.spark.csv").option("header", "true").option("inferSchema", "true")
                                                                 .load(os.environ['HDFS_ROOT'] + "/orig_CMP-test-removalrate.csv"))

URI           = sc._gateway.jvm.java.net.URI
Path          = sc._gateway.jvm.org.apache.hadoop.fs.Path
FileSystem    = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem
Configuration = sc._gateway.jvm.org.apache.hadoop.conf.Configuration
IOUtils = sc._gateway.jvm.org.apache.hadoop.io.IOUtils

dbn_a = os.environ['HDFS_ROOT'] + "/dbn_a.pkl"     #'hdfs://172.16.1.11:8020/user/jiaocheng/wafer_models/dbn_a.pkl'
dbn_b = os.environ['HDFS_ROOT'] + "/dbn_b.pkl"     #'hdfs://172.16.1.11:8020/user/jiaocheng/wafer_models/dbn_b.pkl'
xgb_a = os.environ['HDFS_ROOT'] + "/xgb_a.pkl"     #'hdfs://172.16.1.11:8020/user/jiaocheng/wafer_models/xgb_a.pkl'
xgb_b = os.environ['HDFS_ROOT'] + "/xgb_b.pkl"     #'hdfs://172.16.1.11:8020/user/jiaocheng/wafer_models/xgb_b.pkl'
xgb_lib = os.environ['HDFS_ROOT'] + "/xgb_lib.zip" #'hdfs://172.16.1.11:8020/user/jiaocheng/python_lib/xgb_lib.zip'

#copy data and library files from HDFS to local file system
def save_to_local(hdfs_path):
    local_path = '/tmp/'
    basename = os.path.basename(hdfs_path)
    if os.path.exists(local_path + basename):
        print(local_path + basename + ' has existed')
        return
    fs = FileSystem.get(URI('hdfs://172.16.1.11:8020'), Configuration())
    local_fs = FileSystem.getLocal(Configuration())
    fin = fs.open(Path(hdfs_path))
    fout = local_fs.create(Path(local_path + basename))
    IOUtils.copyBytes(fin, fout, 4096, True)
    print(local_path + basename + ' has done')

save_to_local(dbn_a)
save_to_local(dbn_b)
save_to_local(xgb_a)
save_to_local(xgb_b)
save_to_local(xgb_lib)

save_to_local(os.environ['HDFS_ROOT'] + "/cmp-train-all.csv")
save_to_local(os.environ['HDFS_ROOT'] + "/cmp-training-removalrate.csv")
save_to_local(os.environ['HDFS_ROOT'] + "/CMP_test_data.csv")
save_to_local(os.environ['HDFS_ROOT'] + "/orig_CMP-test-removalrate.csv")
save_to_local(os.environ['HDFS_ROOT'] + "/dbn_lib.zip")
```


3.	Description of time series data:

%pyspark

new_train=train_df.na.drop()
z.show(new_train.describe())
```



4.	Description of Average MRR data:
```
%pyspark

new_train_rm=train_rm_df.na.drop()
z.show(new_train_rm.describe())
```

5.	Remove unnecessary features:
```
%pyspark

new_train=new_train.drop('machine_id').drop('machine_data')
new_test=test_df.drop('machine_id').drop('machine_data')
print "data schema after removing unnecessary features"
new_test.printSchema()
```

6.	Understand Data:
•	Stage – wafer count
```
%pyspark

from pyspark.sql.functions import countDistinct

z.show(new_train_rm.groupBy('stage').agg(countDistinct('wafer_id').alias('wafer_cnt')))
```

•	Chamber – wafer count

```
%pyspark

z.show(new_train.groupBy('chamber').agg(countDistinct('wafer_id').alias('wafer_cnt')).orderBy('chamber'))
```

•	Stage – Chamber count

```
%pyspark

z.show(new_train.groupBy('stage','chamber').agg(countDistinct('wafer_id').alias('wafer_cnt')).orderBy('stage','chamber'))
```

7.	Drop outliers

```
%pyspark

z.show(new_train_rm.filter(new_train_rm['avg_removal_rate']>170))
train_rm1=new_train_rm.filter(new_train_rm['avg_removal_rate']<170)
```

8.	MRR difference between stage A and Stage B
```
%pyspark

from pyspark.sql import functions as F

z.show(train_rm1.groupBy('stage').agg(F.min("avg_removal_rate"), F.avg('avg_removal_rate'),F.max('avg_removal_rate')))
```

9.	Join Average MRR to average features
```
%pyspark

new_avg_train=new_train.groupBy(['wafer_id', 'stage']).avg().drop('avg(wafer_id)').drop('avg(timestamp)')

new_avg_train_rm=train_rm1.groupBy(['wafer_id', 'stage']).avg().drop('avg(wafer_id)')

new_avg_test=new_test.groupBy(['wafer_id', 'stage']).avg().drop('avg(wafer_id)').drop('avg(timestamp)').drop('avg()')

new_avg_test_rm=test_rm_df.groupBy(['wafer_id', 'stage']).avg().drop('avg(wafer_id)')

df=new_avg_train.join(new_avg_train_rm, [new_avg_train.wafer_id==new_avg_train_rm.wafer_id,new_avg_train.stage==new_avg_train_rm.stage], 'inner').drop(new_avg_train_rm.wafer_id).drop(new_avg_train_rm.stage)

df_t=new_avg_test_rm.join(new_avg_test, [new_avg_test.wafer_id==new_avg_test_rm.wafer_id, new_avg_test.stage==new_avg_test_rm.stage]).drop(new_avg_test_rm.wafer_id).drop(new_avg_test_rm.stage)

z.show(df.describe())

```



10.	Chamber vs MRR
```
%pyspark

z.show(df.select('avg(avg_removal_rate)','avg(chamber)','stage'))
```



11.	Retainer Ring Pressure vs MRR
```
%pyspark

z.show(df.select('avg(avg_removal_rate)','avg(retainer_ring_pressure)','stage'))

```

12.	Usage of Backing Film vs MRR
```
%pyspark

z.show(df.select('avg(avg_removal_rate)','avg(usage_of_backing_film)','stage'))

```




13.	Train model - linear regression

o	Logarithm transformation on training & test dataset

```

%pyspark
from pyspark.sql.functions import log2

df1=df
n=len(df.columns)
for names in df1.columns[2:]:
    df1=df.withColumn(names,log2(df[names]+1))
    
df2=df1.na.drop()

df1_t=df_t
for names in df.columns[2:]:
    df1_t=df_t.withColumn(names,log2(df_t[names]+1))

df2_t=df1_t.na.drop()

```

o	Show log transformed training data
```
%pyspark
z.show(df2.describe())

```


o	Show log transformed test data

```
%pyspark
z.show(df_t.describe())
```

14.	Split training data into Group I (i.e., MRR<7) and Group II (i.e., MRR>=7)

```
%pyspark
from pyspark.ml.feature import StringIndexer
from pyspark.mllib.regression import LabeledPoint
import pyspark.ml.regression as lm

# transform categorical feature 'stage' into numeric
indexer = StringIndexer(inputCol='stage', outputCol='stageIndex')
indexed = indexer.fit(df2).transform(df2)
indexed_t = indexer.fit(df2_t).transform(df2_t)

# build labelpoint features
#train= indexed.rdd.map(lambda r:LabeledPoint(r[n-1],r[2:(n-2)]+r[n:n])).toDF(["features","label"])
train1= indexed.filter(indexed['avg(AVG_REMOVAL_RATE)']<7).rdd.map(lambda r:LabeledPoint(r[n-1],r[2:(n-2)]+r[n:n])).toDF(["features","label"])
train2=indexed.filter(indexed['avg(AVG_REMOVAL_RATE)']>=7).rdd.map(lambda r:LabeledPoint(r[n-1],r[2:(n-2)]+r[n:n])).toDF(["features","label"])
test=indexed_t.rdd.map(lambda r:LabeledPoint(r[0],r[3:(n-1)]+r[n:n])).toDF(["features","label"])
```


15.	Train two linear regression models on Group I and II
```
%pyspark
lr = lm.LinearRegression(featuresCol = 'features', labelCol = 'label', maxIter=10, regParam=0.3)
#lrModel=lr.fit(train)
lrModel1=lr.fit(train1)
lrModel2=lr.fit(train2)
```


16.	Predict all test data by Model 1
```
%pyspark
from pyspark.sql.functions import pow

Pred=lrModel1.transform(test).select('label','prediction','features')

z.show(Pred)

```


17.	Predict Group 1 test data by Model 1
```
%pyspark
from pyspark.sql.functions import pow

# split test data into Group I and II
# splitting value 6.1 is based on the observation of model 1 prediction
test1=Pred.filter(Pred.prediction>6.1).select('label','features')
test2=Pred.filter(Pred.prediction<6.1).select('label','features')

# predict Group I by model1
Pred1=lrModel1.transform(test1).select('label','prediction','features')

for names in Pred1.columns[0:2]:
    Pred1=Pred1.withColumn(names,pow(2,Pred1[names])-1)

z.show(Pred1)

```




18.	Predict Group 2 test data by model 2
```
%pyspark
from pyspark.sql.functions import pow


Pred2=lrModel2.transform(test2).select('label','prediction','features')

for names in Pred2.columns[0:2]:
    Pred2=Pred2.withColumn(names,pow(2,Pred2[names])-1)

z.show(Pred2)
```

19.	Coefficients of Linear model 1 and model 2

```
%pyspark
import numpy as np

names=indexed.columns
names=['intercept']+names[2:(n-2)]+names[n:n]
value=np.abs(np.append(lrModel1.weights,lrModel1.intercept))*10000
value=value.tolist()
l = [value]
coe1=sqlContext.createDataFrame(l,names)
z.show(coe1)
value2=np.abs(np.append(lrModel2.weights,lrModel2.intercept))*10000
value2=value2.tolist()
l2 = [value2]
coe2=sqlContext.createDataFrame(l2,names)
z.show(coe2)


#print("Coefficients of model 1: %s" % str(lrModel1.weights))
#print("Intercept of model 1: %s" % str(lrModel1.intercept))

#print("Coefficients of model 2: %s" % str(lrModel1.weights))
#print("Intercept of model 2: %s" % str(lrModel1.intercept))


# Summarize the model over the training set and print out some metrics
#trainingSummary = lrModel.summary
#print("numIterations: %d" % trainingSummary.totalIterations)
#print("objectiveHistory: %s" % str(trainingSummary.objectiveHistory))
#trainingSummary.residuals.show()
#print("RMSE: %f" % trainingSummary.rootMeanSquaredError)
#print("r2: %f" % trainingSummary.r2)

```



20.	RMSE

```

%pyspark

Pre=Pred1.unionAll(Pred2)
pred=Pre.toPandas().to_csv('/tmp/pred.csv')
RMSE=Pre.selectExpr("((label-prediction)*(label-prediction)) as SE").groupBy().agg(F.avg('SE').alias('MSE'))
z.show(RMSE.groupBy('MSE').agg(F.sqrt('MSE').alias('RMSE')))

```



21.	Prediction vs Ground Truth
```
%python
import pandas
import matplotlib.pyplot as plt	
import io
from numpy import *
import pandas as pd

pred=pandas.read_csv('/tmp/pred.csv')
x=pred.label
y=pred.prediction
line = arange(0,300)
plt.plot(line, line, '-')
plt.plot(x, y, 'o')
plt.xlim(50, 200)
plt.ylim(50, 200)
plt.suptitle('Quantitative comparison between predicted\n and measured MRR distributions',fontsize=15, fontweight='bold')
plt.xlabel('Measured average MRR [nm/min]')
plt.ylabel('Predicted average MRR [nm/min]')
img1 = io.BytesIO()                                            #Create StringIO object which named img
plt.savefig(img1, format='svg')                                         #Save the figure in svg format
img1.seek(0)                                                            #Get the memory of the figure
print("%html <div style='width:500pt'>"+img1.getvalue() + "</div>")
img1.close()
plt.close()

```




### **Step 7 : View output**

In this step, we have built and evaluated a linear regression model against CMP data.On successful execution of code, you can view the ouput as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/TENSORFLOW101/assets/images/MRR9.jpeg?raw=true)




### **Step 8 : Select Zeppelin notebook – “Wafer dbn(Deep Belief Network) & xqboost”**

We are using code written in Pyspark, Spark ML and python. 

Following functions are being done by the Code :

1.	Model using Deep Belief Network
2.	Model using extreme gradient boosting
3.	Compare the results


Execute the code by clicking on the icon highlighted as shown in screenshot below:. 
  
![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/TENSORFLOW101/assets/images/MRR10.jpeg?raw=true)

On successful execution of the code, the icon changes to "Finished" as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/TENSORFLOW101/assets/images/MRR11.jpeg?raw=true)


The code is as follows:

1.	Prepare Data:

•	Make packages directory

```
%sh
mkdir /tmp/python_lib

```


•	Extract related packages from zip file
```
%python
import zipfile
zip_ref = zipfile.ZipFile('/tmp/xgb_lib.zip')
zip_ref.extractall('/tmp/python_lib/')
zip_ref.close()
zip_ref = zipfile.ZipFile('/tmp/dbn_lib.zip')
zip_ref.extractall('/tmp/python_lib/')
zip_ref.close()
```
•	Python package needs __init__.py file

```
%sh
touch /tmp/python_lib/dbn_raw/__init__.py

```



2.	Model Deep Belief Network

```

%python
from __future__ import print_function, division
import sys
sys.path.insert(0, '/tmp/python_lib')
import numpy as np
import pickle
from sklearn.metrics.regression import mean_squared_error
from dbn_raw.models_raw import SupervisedDBNRegression
from math import sqrt
from sklearn.preprocessing import MinMaxScaler
import pandas as pd

def normalize(X_train, X_test):
    scaler = MinMaxScaler(copy=False)
    scaler.fit_transform(X_train)
    scaler.transform(X_test)

def get_data(x_path, y_path):
    df_x = pd.read_csv(x_path)
    df_y = pd.read_csv(y_path)
    df_avg = df_x.groupby(['WAFER_ID','STAGE']).mean().reset_index()
    df_merge = df_avg.merge(df_y, on = ['WAFER_ID', 'STAGE']) 
    cols = ['USAGE_OF_BACKING_FILM',
            'USAGE_OF_DRESSER','USAGE_OF_POLISHING_TABLE',
            'USAGE_OF_DRESSER_TABLE','USAGE_OF_MEMBRANE','USAGE_OF_PRESSURIZED_SHEET',
            'PRESSURIZED_CHAMBER_PRESSURE','MAIN_OUTER_AIR_BAG_PRESSURE',                    
            'CENTER_AIR_BAG_PRESSURE','RETAINER_RING_PRESSURE','RIPPLE_AIR_BAG_PRESSURE',
            'SLURRY_FLOW_LINE_A','SLURRY_FLOW_LINE_B',
            'SLURRY_FLOW_LINE_C','WAFER_ROTATION',
            'STAGE_ROTATION','HEAD_ROTATION','EDGE_AIR_BAG_PRESSURE','AVG_REMOVAL_RATE']
    a_idx = df_merge[df_merge.AVG_REMOVAL_RATE <= 100].index.tolist()
    b_idx = df_merge[(df_merge.AVG_REMOVAL_RATE >= 140)&(df_merge.AVG_REMOVAL_RATE <= 165)].index.tolist()
    data = np.array(df_merge[cols])
    a = data[a_idx]
    b = data[b_idx]
    return a[:,0:-1], a[:,-1],b[:,0:-1], b[:,-1]
    

    
X_train_a,Y_train_a,X_train_b,Y_train_b = get_data('/tmp/cmp-train-all.csv', '/tmp/cmp-training-removalrate.csv')
X_test_a,Y_test_a,X_test_b,Y_test_b = get_data('/tmp/CMP_test_data.csv', '/tmp/orig_CMP-test-removalrate.csv')
normalize(X_train_a, X_test_a)
normalize(X_train_b, X_test_b)
regressor_a = SupervisedDBNRegression.load('/tmp/dbn_a.pkl')
Y_pred_a = regressor_a.predict(X_test_a)
regressor_b = SupervisedDBNRegression.load('/tmp/dbn_b.pkl')
Y_pred_b = regressor_b.predict(X_test_b)
print('Done.\nRMSE a: %f' % sqrt(mean_squared_error(Y_test_a, Y_pred_a)))
print('Done.\nRMSE b: %f' % sqrt(mean_squared_error(Y_test_b, Y_pred_b)))
overall_rmse = sqrt((mean_squared_error(Y_test_a, Y_pred_a) * len(Y_test_a) + mean_squared_error(Y_test_b, Y_pred_b) * len(Y_test_b)) / (len(Y_test_a) + len(Y_test_b)))
print('overall_rmse: %f' % overall_rmse)

```

3.	Deep Belief Network Model Results:

```
%python
import matplotlib.pyplot as plt
import io
x = np.concatenate((Y_test_a, Y_test_b), axis=0)
y = np.concatenate((Y_pred_a, Y_pred_b), axis=0)
line = np.arange(0,300)
plt.plot(line, line, '-')
plt.plot(x, y, 'o')
plt.xlim(50, 200)
plt.ylim(50, 200)
plt.suptitle('Quantitative comparison between predicted\n and measured MRR distributions',fontsize=15, fontweight='bold')
plt.xlabel('Measured average MRR [nm/min]')
plt.ylabel('Predicted average MRR [nm/min]')
img1 = io.BytesIO()                                            #Create StringIO object which named img
plt.savefig(img1, format='svg')                                         #Save the figure in svg format
img1.seek(0)                                                            #Get the memory of the figure
print("%html <div style='width:500pt'>" + img1.getvalue() + "</div>")
img1.close()
plt.close()

```


4.	Model of Extreme Gradient Boosting

```

%python
import xgboost as xgb
#regressor_a = pickle.load(open("/tmp/xgb_a.pkl", "rb"))
#regressor_b = pickle.load(open("/tmp/xgb_b.pkl", "rb"))
regressor_a = xgb.XGBRegressor(max_depth=6, n_estimators=1000)
regressor_a.fit(X_train_a, Y_train_a)
regressor_b = xgb.XGBRegressor(max_depth=6, n_estimators=1000)
regressor_b.fit(X_train_b, Y_train_b)
Y_pred_a = regressor_a.predict(X_test_a)
Y_pred_b = regressor_b.predict(X_test_b)
print('Done.\nRMSE a: %f' % sqrt(mean_squared_error(Y_test_a, Y_pred_a)))
print('Done.\nRMSE b: %f' % sqrt(mean_squared_error(Y_test_b, Y_pred_b)))
overall_rmse = sqrt((mean_squared_error(Y_test_a, Y_pred_a) * len(Y_test_a) + mean_squared_error(Y_test_b, Y_pred_b) * len(Y_test_b)) / (len(Y_test_a) + len(Y_test_b)))
print('overall_rmse: %f' % overall_rmse)


```

5.	XGBoost Results

```
%python
import matplotlib.pyplot as plt
import io
x = np.concatenate((Y_test_a, Y_test_b), axis=0)
y = np.concatenate((Y_pred_a, Y_pred_b), axis=0)
line = np.arange(0,300)
plt.plot(line, line, '-')
plt.plot(x, y, 'o')
plt.xlim(50, 200)
plt.ylim(50, 200)
plt.suptitle('Quantitative comparison between predicted\n and measured MRR distributions',fontsize=15, fontweight='bold')
plt.xlabel('Measured average MRR [nm/min]')
plt.ylabel('Predicted average MRR [nm/min]')
img1 = io.BytesIO()                                            #Create StringIO object which named img
plt.savefig(img1, format='svg')                                         #Save the figure in svg format
img1.seek(0)                                                            #Get the memory of the figure
print("%html <div style='width:500pt'>" + img1.getvalue() + "</div>")
img1.close()
plt.close()

```


6.	Features Importance

```
%python
print('xx')
feature_names = ['USAGE_OF_BACKING_FILM',
					'USAGE_OF_DRESSER','USAGE_OF_POLISHING_TABLE',
					'USAGE_OF_DRESSER_TABLE','USAGE_OF_MEMBRANE','USAGE_OF_PRESSURIZED_SHEET',
					'PRESSURIZED_CHAMBER_PRESSURE','MAIN_OUTER_AIR_BAG_PRESSURE',                    
					'CENTER_AIR_BAG_PRESSURE','RETAINER_RING_PRESSURE','RIPPLE_AIR_BAG_PRESSURE',
					'SLURRY_FLOW_LINE_A','SLURRY_FLOW_LINE_B',
					'SLURRY_FLOW_LINE_C','WAFER_ROTATION',
					'STAGE_ROTATION','HEAD_ROTATION','EDGE_AIR_BAG_PRESSURE']
feature_names = np.array(feature_names)
feature_importance_a = regressor_a.feature_importances_
feature_importance_b = regressor_b.feature_importances_
feature_importance_a = 100.0 * (feature_importance_a / feature_importance_a.max())
feature_importance_b = 100.0 * (feature_importance_b / feature_importance_b.max())
sorted_idx_a = np.argsort(feature_importance_a)
sorted_idx_b = np.argsort(feature_importance_b)
pos = np.arange(len(feature_names)) + .5
plt.figure(figsize=(15,9))
plt.subplot(1, 2, 1)
plt.barh(pos, feature_importance_a[sorted_idx_a], 0.5, align='center')
plt.yticks(pos, feature_names[sorted_idx_a])
plt.xlim(0, 105)
plt.xlabel('Relative Importance')
plt.title('Group I Variable Importance')
plt.subplot(1, 2, 2)
plt.barh(pos, feature_importance_b[sorted_idx_b], 0.5, align='center')
plt.yticks(pos, feature_names[sorted_idx_b])
plt.xlim(0, 105)
plt.xlabel('Relative Importance')
plt.title('Group II Variable Importance')
plt.tight_layout()
img1 = io.BytesIO()                                            #Create StringIO object which named img
plt.savefig(img1, format='svg')                                         #Save the figure in svg format
img1.seek(0)                                                            #Get the memory of the figure
print("%html <div style='width:800pt'>" + img1.getvalue() + "</div>")
img1.close()
plt.close()

```

1.	### **Step 8 : Select Zeppelin notebook – “Wafer physical model”**

We are using code written in Pyspark, Spark ML and python. 

Following functions are being done by the Code :

1.	Train physical model
2.	Compare the physical model results


Execute the code by clicking on the icon highlighted as shown in screenshot below:. 
  
![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/TENSORFLOW101/assets/images/MRR10.jpeg?raw=true)

On successful execution of the code, the icon changes to "Finished" as shown in screenshot below:

![Figure](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/TENSORFLOW101/assets/images/MRR11.jpeg?raw=true)


The code is as follows:

1.	Train Physical Model:

```
%python
from __future__ import print_function
from __future__ import division
import pandas as pd
from scipy.optimize import fsolve
import math
from numpy import *
import matplotlib.pyplot as plt
import io

def get_rmse(m, test_data):
    dev = []
    for pr,rt,y in test_data:        
        y_pred = predict(m, (pr,rt))        
        dev.append((y, y_pred))
        #print('%f\t%f' % (y, y_pred))        
    rmse = estimateVal(dev)
    print('RMSE: %f' % rmse)
    dev = array(dev)
    return dev[:,0], dev[:,1], rmse

#Preston equation: MRR = K*(P**a)(V**b)
def get_data2(x_path, y_path):
    df_train = pd.read_csv(x_path)
    df_rm = pd.read_csv(y_path)
    df_train['PRESSURE'] = (df_train.PRESSURIZED_CHAMBER_PRESSURE + df_train.MAIN_OUTER_AIR_BAG_PRESSURE + df_train.CENTER_AIR_BAG_PRESSURE
                                                                  + df_train.RETAINER_RING_PRESSURE 
								  + df_train.RIPPLE_AIR_BAG_PRESSURE + df_train.EDGE_AIR_BAG_PRESSURE )
    df_train['ROTATION'] = df_train.WAFER_ROTATION + df_train.STAGE_ROTATION
    df_avg = df_train.groupby(['WAFER_ID','STAGE']).agg({'PRESSURE': 'mean','ROTATION': 'mean'}).reset_index()    
    df_merge = df_avg.merge(df_rm, on = ['WAFER_ID', 'STAGE']) 
    cols = ['PRESSURE', 'ROTATION', 'AVG_REMOVAL_RATE']
    a_idx = df_merge[df_merge.AVG_REMOVAL_RATE <= 100].index.tolist()
    # b_idx = df_merge[(df_merge.AVG_REMOVAL_RATE >= 140)&(df_merge.AVG_REMOVAL_RATE <= 165)&
            # (df_merge.PRESSURE != 0)].index.tolist()
    b_idx = df_merge[(df_merge.AVG_REMOVAL_RATE >= 140)&(df_merge.AVG_REMOVAL_RATE <= 165)].index.tolist()
    data = array(df_merge[cols])
    a = data[a_idx]
    b = data[b_idx]
    return a, b


def myFunction(z, param):
   k, a, b = z
   #func. 
   f = empty((3))
   i = 0
   for (x,y,z) in param:       
       f[i] = k * math.pow(x, a) * math.pow(y, b) - z
       i = i + 1
   return f

def build_model2(dataList):
    kArray = []
    aArray = []
    bArray = []
    zGuess = array([1,1,1])
    tupleList = []
    i = 0    
    for arr in  dataList:       
        if 0  == int(arr[0]) or 0  == int(arr[1]):            
            continue        
        i = i + 1        
        tupleList.append(arr)
        if i%3 == 0:
            k, a, b  = fsolve(myFunction, zGuess, tupleList)
            
            if ( k >= 150) and ( k<= 700):
                kArray.append(k)
                aArray.append(a)
                bArray.append(b)               
            tupleList = []    
    kAvg = sum(kArray)/len(kArray)
    aAvg = sum(aArray)/len(aArray)
    bAvg = sum(bArray)/len(bArray)
    return (kAvg, aAvg, bAvg)
    

def predict(m, d):
    k, a, b = m    
    x, y = d
    a = 0 if 0 == int(x) else a
    b = 0 if 0 == int(y) else b
    # if 0 == int(x) or 0 == int(y):
       # return 0
    return  k * math.pow(x, a) * math.pow(y, b)



def estimateVal(tList):
    sumVal = 0
    for p, a in tList:
        #print('p, a',p,a)
        sumVal = sumVal + math.pow((p -a), 2)
        #print('p, a, sum:',p,a,sumVal)
    return math.sqrt(sumVal/len(tList))
    
    

train_a, train_b = get_data2('/tmp/cmp-train-all.csv', '/tmp/cmp-training-removalrate.csv')
test_a, test_b = get_data2('/tmp/CMP_test_data.csv', '/tmp/orig_CMP-test-removalrate.csv')    
m_a = (356.63721473818919, -0.016890878358881443, -0.32380617498813929)
m_b = (225.69876624236517, -0.03755748580984368, -0.016337749763835042)
#m = build_model2(train_b)    

print('formula MRR = K*(P**a)(V**b) group A coneffient (k, a, b) :', m_a)
print('formula MRR = K*(P**a)(V**b) group B coneffient (k, a, b) :', m_b)
xa,ya,rmse_a = get_rmse(m_a, test_a)
xb,yb,rmse_b = get_rmse(m_b, test_b)
overall_rmse = math.sqrt(((rmse_a**2) * len(test_a) + (rmse_b**2) * len(test_b)) / (len(test_a) + len(test_b)))
print('overall_rmse: %f' % overall_rmse)

```


2.	Physical Model Results
```
%python
x = concatenate((xa, xb), axis=0)
y = concatenate((ya, yb), axis=0)
line = arange(0,300)
plt.plot(line, line, '-')
plt.plot(x, y, 'o')
plt.xlim(50, 300)
plt.ylim(50, 300)
plt.suptitle('Quantitative comparison between predicted\n and measured MRR distributions',fontsize=15, fontweight='bold')
plt.xlabel('Measured average MRR [nm/min]')
plt.ylabel('Predicted average MRR [nm/min]')
img1 = io.BytesIO()                                            #Create StringIO object which named img
plt.savefig(img1, format='svg')                                         #Save the figure in svg format
img1.seek(0)                                                            #Get the memory of the figure
print("%html <div style='width:500pt'>" + img1.getvalue() + "</div>")
img1.close()
plt.close()
```

## LESSONS LEARNT

1.	How to write and execute python code using Apache Zeppelin.

2.	How to predict MRR (Material Removal Rate) of wafers by modelling CMP(Chemical Mechanical Planarization). 



# **Congratulations! You have successfully completed the Learning Lab!**
