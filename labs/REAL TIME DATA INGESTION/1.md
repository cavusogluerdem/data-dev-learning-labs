# REAL TIME DATA INGESTION

**Stream Processing Platform Used**: Apache Kafka

**Executed on**: Hadoop Big Data Platform


## OVERVIEW

This learning lab can be used as a guide to get a high level understanding on the process of ingesting data real time in to Hadoop environment. 

We will be using DevNet Data Learning Platform referred as "DLP" during the course. 

Please refer the diagram shown below to get a high level understanding:

[to be checked]


## PRE-REQUISITES

1. Install Chrome Browser.

2. Basic Knowledge of data storage on Hadoop.

3. Basic knowledge of Apache Kafka.

4. Obtain access to Data Learning Platform -
                                                          Refer: https://github.com/CiscoDevNet/data-dev-learning-labs/blob/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/HowToAccessDTLP.md


## LEARNING OBJECTIVES

1. To get familiarized with the DLP (Data Learning Platform).

2. To get familiarized with the ways to ingest real time data to HDFS.

3. To get familiarized with the ways to get network data from HDFS. 

4. To get familiarized visualization data from DLP platform.


## TERMINOLOGIES USED


### WHAT IS NETWORK DATA? - AN INTRODUCTION

The Computer network is a telecommunication process which allows computers or devices to exchange data between each other using data pipeline and those devices that are controlled by wired or wireless medium. Those devices are kept alive by exchanging data between each other in a continuous way. 

These network data provide the inside details about communication performance between of two devices that are communicating. We can extract lots of valuable information from those data set if we can capture those data in real time. 

## APACHE NIFI? - AN INTRODUCTION

NiFi was built to automate the flow of data between systems. The termÿdataflowÿhere refers to automated and managed flow of information between systems. Some of the high-level challenges of dataflow include:
1. Systems fail : Networks fail, disks fail, software crashes, people make mistakes.
2. Data access exceeds capacity to consume : Sometimes a given data source can outpace some part of the processing or delivery chain - it only takes one weak-link to have an issue.
3. Boundary conditions are mere suggestions : You will invariably get data that is too big, too small, too fast, too slow, corrupt, wrong, or in the wrong format.
4. What is noise one day becomes signal the next : Priorities of an organization change - rapidly. Enabling new flows and changing existing ones must be fast.
5. Systems evolve at different rates : The protocols and formats used by a given system can change anytime and often irrespective of the systems around them. Dataflow exists to connect what is essentially a massively distributed system of components that are loosely or not-at-all designed to work together.
6. Compliance and security : Laws, regulations, and policies change. Business to business agreements change. System to system and system to user interactions must be secure, trusted, accountable.
7. Continuous improvement occurs in production : It is often not possible to come even close to replicating production environments in the lab.

NiFi is built to help tackle these modern dataflow challenges.

For more details, please refer:
http://nifi.apache.org/docs/nifi-docs/html/overview.html

## APACHE KAFKA? - AN INTRODUCTION

Kafka is a distributed streaming platform that is designed to be fast, scalable, and durable. It has 3 key capabilities:

1.	It lets you publish and subscribe to streams of records. In this respect,    it is similar to a message queue or enterprise messaging system.
2.	It lets you store streams of records in a fault-tolerant way.
3.	It lets you process streams of records as they occur.

It is used for two broad classes of application:

1.	Building real-time streaming data pipelines that reliably get data between systems or applications
2.	Building real-time streaming applications that transform or react to the streams of data

For more details, please refer:
https://kafka.apache.org/


## PROCESS OVERVIEW 

![alt-tag](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/WORD%20COUNT%20USING%20SCALA%20WITH%20APACHE%20SPARK/assets/images/Process.jpeg?raw=true)

Please follow the steps given below to launch the workspace and execute the lab.

### **Step 1 : Select Learning lab from DLP**

After launching the DLP dashboard page, navigate to learning labs tab. Select the learning lab "Real Time Data Ingestion" and click on "Start" button as shown in screenshot below:

![alt-tag](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/REAL%20TIME%20DATA%20INGESTION/assets/images/RTDI1.jpeg?raw=true)


### **Step 2 : Workspace Page**

On click of Start button, user will be navigated to a workspace page where all the components - IDE, Tools and Microservices required to execute the program are available. Please refer the screenshot below:

![alt-tag](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/REAL%20TIME%20DATA%20INGESTION/assets/images/RTDI2.jpeg?raw=true)

### **Step 3 : Input and Output Configuration**

Click on the configuration settings button provided in "Tools and Microservices" column. Please refer screenshot below. The input and output configuration existing for the learning lab is shown.

![alt-tag](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/REAL%20TIME%20DATA%20INGESTION/assets/images/RTDI4.jpeg?raw=true)

### **Step 4 : Start Tasks in workspace**

Points to Note:

* Start the tasks in "Tools and Microservices" column. Please refer screenshot below:

![alt-tag](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/REAL%20TIME%20DATA%20INGESTION/assets/images/RTDI3.jpeg?raw=true)

* Once started, the colour of the icon changes to green as shown in screen below:

![alt-tag](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/REAL%20TIME%20DATA%20INGESTION/assets/images/RTDI5.jpeg?raw=true)

* If the task is in stopped status then click on it to start again. 

Data source section will get real-time traffic from traffic server and feed to Kafka cluster. Stream2HDFS allows to save the real-time data to HDFS. From HDFS, we can visualize the data using visualization tool like Tableau.

The generated output file is shown in section - "File List"

![alt-tag](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/REAL%20TIME%20DATA%20INGESTION/assets/images/RTDI6.jpeg?raw=true)

Click on the button with an eye symbol to view the network data as shown in screenshot below.

![alt-tag](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/REAL%20TIME%20DATA%20INGESTION/assets/images/RTDI7.jpeg?raw=true)

PS: The window needs to be refreshed if the output file is not seen.
</br>


### **Step 5 : Stop Tasks in workspace**

After viewing the network data, stop the task by clicking on **Stop** button in "Tools and Microservices" column. Please refer screenshot below:

![alt-tag](https://raw.githubusercontent.com/CiscoDevNet/data-dev-learning-labs/master/labs/REAL%20TIME%20DATA%20INGESTION/assets/images/RTDI5.jpeg?raw=true)

## LESSONS LEARNT:

1. How to ingest real time streaming data.

2. How to visualize the data from HDFS via DLP platform.


# **Congratulations! You have successfully completed the Learning Lab!**

